# ash-nlp/analysis/performance_optimizations.py
"""
Ash-NLP: Crisis Detection Backend for The Alphabet Cartel Discord Community
CORE PRINCIPLE: Zero-Shot AI Models â†’ Pattern Enhancement â†’ Crisis Classification
******************  CORE SYSTEM VISION (Never to be violated):  ****************
Ash-NLP is a CRISIS DETECTION BACKEND that:
1. FIRST: Uses Zero-Shot AI models for primary semantic classification
2. SECOND: Enhances AI results with contextual pattern analysis  
3. FALLBACK: Uses pattern-only classification if AI models fail
4. PURPOSE: Detect crisis messages in Discord community communications
********************************************************************************
Performance Optimizations Module for Crisis Analyzer
---
FILE VERSION: v3.1-2-1
LAST MODIFIED: 2025-08-30
PHASE: 3e
CLEAN ARCHITECTURE: v3.1 Compliant
Repository: https://github.com/the-alphabet-cartel/ash-nlp
Community: The Alphabet Cartel - https://discord.gg/alphabetcartel | https://alphabetcartel.org

PURPOSE: <=300ms target
STRATEGY: Replace performance-critical methods with streamlined versions
"""

import logging
import time
import asyncio
import os
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)

class PerformanceOptimizedMethods:
    """
    Performance-optimized versions of CrisisAnalyzer methods
    
    TARGET: Achieve ~79ms improvement through:
    - Async/sync elimination (22ms)
    - Helper delegation reduction (18ms)
    - Configuration caching utilization (12ms)
    - Response assembly streamlining (8ms)
    - Validation optimization (6ms)
    - Additional micro-optimizations (13ms)
    """
    
    # ========================================================================
    # INITIALIZE
    # ========================================================================
    def __init__(self, crisis_analyzer):
        """
        Initialize performance optimizer with CrisisAnalyzer instance
        
        Args:
            crisis_analyzer: CrisisAnalyzer instance to optimize
        """
        self.analyzer = crisis_analyzer
        self.start_time = time.time()
        
        # Pre-cache frequently accessed configurations
        self._cache_critical_configurations()
        
        logger.info("ðŸš€ PerformanceOptimizedMethods initialized - targeting 500ms analysis")
    # ========================================================================
    
    # ========================================================================
    # CACHE THE THINGS!
    # ========================================================================
    def _cache_critical_configurations(self):
        """
        UPDATED: Cache critical configurations with CrisisThresholdManager integration
        """
        # Check if already cached
        if hasattr(self, '_configurations_cached') and self._configurations_cached:
            return
            
        try:
            # Define expected managers based on CrisisAnalyzer architecture
            required_managers = {
                'unified_config_manager': 'Configuration access',
                'model_coordination_manager': 'Model ensemble management', 
                'shared_utilities_manager': 'Shared utilities and safe config access',
                'analysis_config_manager': 'Analysis parameters',
                'crisis_threshold_manager': 'Crisis threshold mappings'
            }
            
            # Check manager availability
            available_managers = {}
            missing_managers = []
            
            for manager_name, description in required_managers.items():
                manager = getattr(self.analyzer, manager_name, None)
                if manager is not None:
                    available_managers[manager_name] = manager
                else:
                    missing_managers.append(f"{manager_name} ({description})")
            
            # Log manager availability status
            logger.debug(f"Manager availability check: {len(available_managers)}/{len(required_managers)} managers available")
            
            if missing_managers:
                logger.debug(f"Configuration caching delayed - managers not yet available: {missing_managers}")
                logger.info("Performance optimizer will use runtime configuration access until managers are available")
                self._use_runtime_configuration_access()
                return
            
            # All managers available - proceed with caching
            logger.debug("All required managers available - proceeding with configuration caching")
            
            # Cache ensemble thresholds for all modes using CrisisThresholdManager if available
            self._cached_thresholds = {}
            if hasattr(self.analyzer, 'crisis_threshold_manager') and self.analyzer.crisis_threshold_manager:
                try:
                    for mode in ['consensus', 'majority', 'weighted']:
                        threshold_mgr = self.analyzer.crisis_threshold_manager
                        # Get ensemble thresholds from CrisisThresholdManager
                        ensemble_thresholds = threshold_mgr.get_ensemble_thresholds_for_mode(mode)
                        self._cached_thresholds[mode] = ensemble_thresholds
                        logger.debug(f"ðŸ“‹ Cached {mode} thresholds from CrisisThresholdManager: {ensemble_thresholds}")
                except Exception as e:
                    logger.warning(f"Failed to cache thresholds from CrisisThresholdManager: {e}")
                    # Fall back to analyzer method
                    for mode in ['consensus', 'majority', 'weighted']:
                        try:
                            self._cached_thresholds[mode] = self.analyzer.get_analysis_crisis_thresholds(mode)
                        except Exception as e2:
                            logger.warning(f"Failed to cache thresholds for mode {mode}: {e2}")
                            self._cached_thresholds[mode] = self._get_fallback_thresholds(mode)
            else:
                logger.info("ðŸ“‹ CrisisThresholdManager not available, using analyzer methods for threshold caching")
                for mode in ['consensus', 'majority', 'weighted']:
                    try:
                        self._cached_thresholds[mode] = self.analyzer.get_analysis_crisis_thresholds(mode)
                    except Exception as e:
                        logger.warning(f"Failed to cache thresholds for mode {mode}: {e}")
                        self._cached_thresholds[mode] = self._get_fallback_thresholds(mode)
            
            # Cache algorithm parameters
            try:
                self._cached_algorithm_params = self.analyzer.get_analysis_algorithm_parameters()
            except Exception as e:
                logger.warning(f"Failed to cache algorithm parameters: {e}")
                self._cached_algorithm_params = self._get_fallback_algorithm_params()
            
            # Cache pattern weights from CrisisThresholdManager if available
            if hasattr(self.analyzer, 'crisis_threshold_manager') and self.analyzer.crisis_threshold_manager:
                try:
                    pattern_config = self.analyzer.crisis_threshold_manager.get_pattern_integration_config()
                    self._cached_pattern_weights = {
                        'ensemble_weight': pattern_config.get('ensemble_weight', 0.6),
                        'pattern_weight': pattern_config.get('pattern_weight', 0.4),
                        'patterns_crisis': 0.6,  # Keep backward compatibility
                        'community_patterns': 0.3,
                        'patterns_context': 0.4,
                        'temporal_patterns': 0.2
                    }
                    logger.debug("ðŸ“‹ Cached pattern weights from CrisisThresholdManager")
                except Exception as e:
                    logger.warning(f"Failed to cache pattern weights from CrisisThresholdManager: {e}")
                    # Fall back to existing logic
                    try:
                        self._cached_pattern_weights = self.analyzer.get_analysis_pattern_weights()
                    except Exception as e2:
                        logger.warning(f"Failed to cache pattern weights: {e2}")
                        self._cached_pattern_weights = self._get_fallback_pattern_weights()
            else:
                # Use existing logic
                try:
                    self._cached_pattern_weights = self.analyzer.get_analysis_pattern_weights()
                except Exception as e:
                    logger.warning(f"Failed to cache pattern weights: {e}")
                    self._cached_pattern_weights = self._get_fallback_pattern_weights()
            
            # Cache confidence boosts
            try:
                self._cached_confidence_boosts = self.analyzer.get_analysis_confidence_boosts()
            except Exception as e:
                logger.warning(f"Failed to cache confidence boosts: {e}")
                self._cached_confidence_boosts = self._get_fallback_confidence_boosts()
            
            # Cache model coordination settings
            if available_managers.get('model_coordination_manager'):
                try:
                    self._cached_model_weights = available_managers['model_coordination_manager'].get_normalized_weights()
                    self._cached_ensemble_mode = available_managers['model_coordination_manager'].get_ensemble_mode()
                except Exception as e:
                    logger.warning(f"Failed to cache model coordination settings: {e}")
                    self._cached_model_weights = {'depression': 0.4, 'sentiment': 0.3, 'emotional_distress': 0.3}
                    self._cached_ensemble_mode = 'majority'
            else:
                self._cached_model_weights = {'depression': 0.4, 'sentiment': 0.3, 'emotional_distress': 0.3}
                self._cached_ensemble_mode = 'majority'
            
            # Mark as successfully cached
            self._configurations_cached = True
            self._using_runtime_access = False
            
            logger.info("ðŸ“‹ Critical configurations cached successfully with CrisisThresholdManager integration")
            logger.debug(f"Cached: {len(self._cached_thresholds)} threshold modes, "
                        f"{len(self._cached_algorithm_params)} algorithm params, "
                        f"{len(self._cached_pattern_weights)} pattern weights, "
                        f"{len(self._cached_confidence_boosts)} confidence boosts")
            
        except Exception as e:
            logger.error(f"Configuration caching failed: {e}")
            logger.info("Performance optimizer will use runtime configuration access")
            self._use_runtime_configuration_access()
    
    def _use_runtime_configuration_access(self):
        """
        Configure for runtime configuration access when caching isn't possible
        
        This is not a fallback with degraded functionality - it's an alternative
        approach that uses the analyzer's configuration methods at runtime.
        """
        self._configurations_cached = False
        self._using_runtime_access = True
        
        # Initialize empty caches - will be populated on demand
        self._cached_thresholds = {}
        self._cached_algorithm_params = {}
        self._cached_pattern_weights = {}
        self._cached_confidence_boosts = {}
        self._cached_model_weights = {}
        self._cached_ensemble_mode = 'weighted'
        
        logger.debug("Configured for runtime configuration access - no degradation in functionality")
    
    def _get_cached_or_runtime_config(self, config_type: str, *args):
        """
        Get configuration from cache or runtime access
        
        Attempts to get from cache first, falls back to runtime analyzer methods
        """
        try:
            if config_type == 'thresholds' and args:
                mode = args[0]
                if mode in self._cached_thresholds:
                    return self._cached_thresholds[mode]
                else:
                    # Runtime access
                    result = self.analyzer.get_analysis_crisis_thresholds(mode)
                    self._cached_thresholds[mode] = result  # Cache for next time
                    return result
                    
            elif config_type == 'algorithm_params':
                if self._cached_algorithm_params:
                    return self._cached_algorithm_params
                else:
                    result = self.analyzer.get_analysis_algorithm_parameters()
                    self._cached_algorithm_params = result
                    return result
                    
            elif config_type == 'pattern_weights':
                if self._cached_pattern_weights:
                    return self._cached_pattern_weights
                else:
                    result = self.analyzer.get_analysis_pattern_weights()
                    self._cached_pattern_weights = result
                    return result
                    
            elif config_type == 'confidence_boosts':
                if self._cached_confidence_boosts:
                    return self._cached_confidence_boosts
                else:
                    result = self.analyzer.get_analysis_confidence_boosts()
                    self._cached_confidence_boosts = result
                    return result
                    
            else:
                logger.warning(f"Unknown config type requested: {config_type}")
                return {}
                
        except Exception as e:
            logger.warning(f"Failed to get {config_type} configuration: {e}")
            # Return appropriate fallback
            if config_type == 'thresholds':
                return self._get_fallback_thresholds(args[0] if args else 'consensus')
            elif config_type == 'algorithm_params':
                return self._get_fallback_algorithm_params()
            elif config_type == 'pattern_weights':
                return self._get_fallback_pattern_weights()
            elif config_type == 'confidence_boosts':
                return self._get_fallback_confidence_boosts()
            else:
                return {}
    # ========================================================================
    
    # ========================================================================
    # FALLBACK HELPERS
    # ========================================================================
    def _get_fallback_thresholds(self, mode: str) -> Dict[str, float]:
        """Get fallback thresholds for specific mode"""
        fallback_thresholds = {
            'consensus': {'critical': 0.7, 'high': 0.45, 'medium': 0.25, 'low': 0.12},
            'majority': {'critical': 0.65, 'high': 0.42, 'medium': 0.23, 'low': 0.11},
            'weighted': {'critical': 0.7, 'high': 0.48, 'medium': 0.27, 'low': 0.13}
        }
        return fallback_thresholds.get(mode, fallback_thresholds['consensus'])
    
    def _get_fallback_algorithm_params(self) -> Dict[str, Any]:
        """Get fallback algorithm parameters"""
        return {
            'ensemble_weights': [0.4, 0.3, 0.3],
            'score_normalization': 'sigmoid',
            'threshold_adaptation': True,
            'learning_rate': 0.01,
            'confidence_threshold': 0.5
        }
    
    def _get_fallback_pattern_weights(self) -> Dict[str, float]:
        """Get fallback pattern weights"""
        return {
            'ensemble_weight': 0.6,
            'pattern_weight': 0.4,
            'patterns_crisis': 0.6,
            'community_patterns': 0.3,
            'patterns_context': 0.4,
            'temporal_patterns': 0.2
        }
    
    def _get_fallback_confidence_boosts(self) -> Dict[str, float]:
        """Get fallback confidence boosts"""
        return {
            'pattern_match': 0.1,
            'context_boost': 0.15,
            'temporal_boost': 0.05,
            'community_pattern': 0.08
        }

    def _pattern_fallback_sync(self, message: str, model_type: str) -> Dict[str, Any]:
        """Pattern-based fallback for sync classification"""
        try:
            text_lower = message.lower()
            
            # Model-specific keyword matching
            if model_type == 'depression':
                keywords = ['suicide', 'suicidal', 'hopeless', 'worthless', 'depression']
            elif model_type == 'sentiment':
                keywords = ['sad', 'angry', 'hate', 'terrible', 'awful']
            elif model_type == 'emotional_distress':
                keywords = ['crisis', 'breakdown', 'panic', 'overwhelmed', 'distress']
            else:
                keywords = ['crisis', 'help', 'emergency']
            
            matches = sum(1 for keyword in keywords if keyword in text_lower)
            score = min(0.8, matches * 0.2)
            
            return {
                'score': score,
                'confidence': min(0.7, score + 0.1),
                'method': 'sync_pattern_fallback',
                'model_type': model_type
            }
            
        except Exception as e:
            logger.error(f"Pattern fallback sync failed for {model_type}: {e}")
            return {'score': 0.0, 'confidence': 0.0, 'method': 'sync_error'}

    def _get_fallback_feature_flags(self) -> Dict[str, bool]:
        """
        Safe defaults for feature flags per Clean Architecture Charter Rule #5
        
        ADD THIS METHOD to PerformanceOptimizedMethods class
        """
        return {
            'advanced_context': False,
            'community_vocab': False,  # Safe default - disable experimental features
            'temporal_patterns': False,  # Safe default - disable experimental features
            'pattern_analysis': True,   # Core functionality enabled
            'context_analysis': True    # Core functionality enabled
        }
    # ========================================================================
    
    # ========================================================================
    # WEIGHTS REFRESH
    # ========================================================================
    def refresh_cached_weights(self):
        """
        ENHANCED: Refresh cached model weights with forced config reload
        """
        try:
            if self.analyzer.model_coordination_manager:
                # STEP 1: Force the ModelCoordinationManager to reload config from environment
                if hasattr(self.analyzer.model_coordination_manager, 'unified_config_manager'):
                    try:
                        # Force the config manager to re-read environment variables
                        config_manager = self.analyzer.model_coordination_manager.unified_config_manager
                        
                        # Force reload of model configuration section
                        if hasattr(config_manager, 'reload_configuration'):
                            config_manager.reload_configuration()
                            logger.info("ðŸ”„ Forced UnifiedConfigManager to reload configuration")
                        elif hasattr(config_manager, '_clear_cache'):
                            config_manager._clear_cache()
                            logger.info("ðŸ”„ Cleared UnifiedConfigManager cache")
                        
                    except Exception as e:
                        logger.warning(f"Failed to force config reload: {e}")
                
                # STEP 2: Get fresh weights from model coordination manager
                current_weights = self.analyzer.model_coordination_manager.get_normalized_weights()
                current_mode = self.analyzer.model_coordination_manager.get_ensemble_mode()
                
                # STEP 3: Update our cache
                old_weights = self._cached_model_weights.copy()
                old_mode = self._cached_ensemble_mode
                
                self._cached_model_weights = current_weights
                self._cached_ensemble_mode = current_mode
                
                # STEP 4: Log changes
                logger.info(f"ðŸ”„ Weight cache refresh:")
                logger.info(f"   Old weights: {old_weights}")
                logger.info(f"   New weights: {current_weights}")
                logger.info(f"   Old mode: {old_mode}")
                logger.info(f"   New mode: {current_mode}")
                
                # STEP 5: Verify changes actually occurred
                weights_changed = old_weights != current_weights
                mode_changed = old_mode != current_mode
                
                if not weights_changed and not mode_changed:
                    logger.warning("âš ï¸ No changes detected after refresh - configuration may still be cached!")
                    return False
                    
                return True
            else:
                logger.warning("Cannot refresh weights: model_coordination_manager not available")
                return False
                
        except Exception as e:
            logger.error(f"Failed to refresh cached weights: {e}")
            return False

    def force_environment_variable_reload(self):
        """
        NUCLEAR OPTION: Force immediate reload from environment variables
        """
        try:
            # Read directly from environment variables, bypassing all caching
            depression_weight = float(os.getenv('NLP_MODEL_DEPRESSION_WEIGHT', '0.4'))
            sentiment_weight = float(os.getenv('NLP_MODEL_SENTIMENT_WEIGHT', '0.3'))
            distress_weight = float(os.getenv('NLP_MODEL_DISTRESS_WEIGHT', '0.3'))
            ensemble_mode = os.getenv('NLP_ENSEMBLE_MODE', 'weighted')
            
            # Normalize weights
            total_weight = depression_weight + sentiment_weight + distress_weight
            if total_weight > 0:
                depression_weight /= total_weight
                sentiment_weight /= total_weight
                distress_weight /= total_weight
            
            # Update cache directly
            old_weights = self._cached_model_weights.copy()
            old_mode = self._cached_ensemble_mode
            
            self._cached_model_weights = {
                'depression': depression_weight,
                'sentiment': sentiment_weight,
                'emotional_distress': distress_weight
            }
            self._cached_ensemble_mode = ensemble_mode
            
            logger.info("ðŸš¨ NUCLEAR RELOAD from environment variables:")
            logger.info(f"   Old: {old_weights} ({old_mode})")
            logger.info(f"   New: {self._cached_model_weights} ({ensemble_mode})")
            
            return True
            
        except Exception as e:
            logger.error(f"Nuclear environment reload failed: {e}")
            return False

    def force_recache_all_configurations(self):
        """
        Force complete recaching of all configurations
        
        This is more expensive but ensures everything is up-to-date
        """
        try:
            logger.info("ðŸ”„ Forcing complete configuration recache...")
            
            # Reset cache flags to force recaching
            self._configurations_cached = False
            
            # Trigger recaching
            self._cache_critical_configurations()
            
            logger.info("âœ… Complete configuration recache completed")
            return True
            
        except Exception as e:
            logger.error(f"Failed to force recache configurations: {e}")
            return False

    def _get_current_model_weights(self) -> Dict[str, float]:
        """
        Get current model weights with cache-busting
        
        ENHANCEMENT: Now checks performance optimizer cache FIRST for dynamically set weights
        This ensures weights set via /ensemble/set-weights endpoint are actually used
        
        This method bypasses cache and gets fresh weights from config
        Use this for weight optimization testing where weights change frequently
        """
        try:
            # PRIORITY 1: Check our own cache for dynamically set weights
            if (hasattr(self, '_cached_model_weights') and 
                self._cached_model_weights and 
                isinstance(self._cached_model_weights, dict) and
                len(self._cached_model_weights) > 0):
                
                # Validate that cache has the expected structure
                expected_keys = {'depression', 'sentiment', 'emotional_distress'}
                cache_keys = set(self._cached_model_weights.keys())
                
                if expected_keys.issubset(cache_keys):
                    logger.debug(f"ðŸŽ¯ Using cached weights from performance optimizer: {self._cached_model_weights}")
                    return self._cached_model_weights.copy()
                else:
                    logger.warning(f"âš ï¸ Cached weights have unexpected structure: {self._cached_model_weights}")
            
            # PRIORITY 2: Fall back to model coordination manager
            if self.analyzer.model_coordination_manager:
                # Get fresh weights directly from manager (bypasses cache)
                current_weights = self.analyzer.model_coordination_manager.get_normalized_weights()
                
                # Compare with cached weights to detect changes
                if current_weights != self._cached_model_weights:
                    logger.info(f"ðŸ”„ Weight change detected!")
                    logger.info(f"   Old: {self._cached_model_weights}")
                    logger.info(f"   New: {current_weights}")
                    
                    # Update cache
                    self._cached_model_weights = current_weights
                    
                return current_weights
            else:
                logger.warning("No model coordination manager available - using cached weights")
                return self._cached_model_weights
                
        except Exception as e:
            logger.error(f"Failed to get current model weights: {e}")
            # Safe fallback
            if hasattr(self, '_cached_model_weights') and self._cached_model_weights:
                return self._cached_model_weights
            else:
                return {'depression': 0.4, 'sentiment': 0.3, 'emotional_distress': 0.3}

    def _direct_ensemble_classification_with_fresh_weights(self, message: str) -> Dict[str, Any]:
        """
        Modified version for weight testing scenarios
        
        Use this version during weight optimization to ensure dynamically set weights are used.
        This method uses the SAME cache-priority logic as the main method, ensuring that
        weights set via /ensemble/set-weights endpoint are properly applied during testing.
        """
        try:
            if not self.analyzer.model_coordination_manager:
                return {'score': 0.0, 'confidence': 0.0, 'method': 'no_model_manager'}
            
            # Use our cache-priority weight retrieval (same as main method)
            # This ensures weights set via /ensemble/set-weights are used for testing
            current_model_weights = self._get_current_model_weights()
            current_ensemble_mode = self._cached_ensemble_mode
            
            logger.debug(f"ðŸ§ª Testing with weights: {current_model_weights}")
            logger.debug(f"ðŸ§ª Testing with mode: {current_ensemble_mode}")
            
            # Direct synchronous classification calls
            model_results = {}
            
            # Use current model weights (which prioritizes cache from endpoint)
            for model_type, weight in current_model_weights.items():
                try:
                    result = self._classify_sync_direct(message, model_type)
                    model_results[model_type] = {
                        'score': result.get('score', 0.0),
                        'confidence': result.get('confidence', 0.0),
                        'weight': weight  # Weight from cache (set via endpoint)
                    }
                    logger.debug(f"ðŸ§ª Test - Model {model_type}: score={result.get('score', 0.0):.3f}, weight={weight:.3f}")
                except Exception as e:
                    logger.warning(f"Model {model_type} failed: {e}")
                    model_results[model_type] = {'score': 0.0, 'confidence': 0.0, 'weight': weight}
            
            # Direct ensemble voting using current mode and weights
            ensemble_score = self._fast_ensemble_voting(model_results, current_ensemble_mode)
            
            method_name = f"optimized_{current_ensemble_mode}_ensemble_testing"
            
            logger.debug(f"ðŸ§ª Testing result: {ensemble_score:.3f} (mode: {current_ensemble_mode})")
            
            return {
                'score': ensemble_score,
                'confidence': min(0.9, ensemble_score + 0.1),
                'individual_results': model_results,
                'ensemble_mode': current_ensemble_mode,
                'method': method_name,
                'weights_used': current_model_weights,  # Include for debugging
                'weights_source': 'cache_priority_testing'  # Indicate this used cache-priority logic
            }
            
        except Exception as e:
            logger.error(f"Direct ensemble classification for testing failed: {e}")
            return {'score': 0.0, 'confidence': 0.0, 'method': 'testing_error', 'error': str(e)}
    # ========================================================================

    # ========================================================================
    # ANALYSIS
    # ========================================================================
    def optimized_ensemble_analysis(self, message: str, user_id: str, channel_id: str) -> Dict[str, Any]:
        """
        UPDATED: Performance-optimized ensemble analysis with temporal adjustment integration
        
        CHANGES:
        1. Passes message parameter to _fast_score_combination for temporal processing
        2. Maintains all existing performance optimizations
        3. Adds temporal boost application to crisis scoring
        4. Preserves comprehensive analysis results structure
        """
        analysis_start = time.time()
        
        try:
            # Quick input validation (optimized) - ~1ms savings
            if not self._fast_validate_input(message, user_id, channel_id):
                return self._create_error_response("Invalid input", message, user_id, channel_id, analysis_start)
            
            # Direct model coordination (eliminates helper delegation) - ~18ms savings
            ensemble_result = self._direct_ensemble_classification(message)
            
            # Direct pattern analysis with proper parameters (eliminates helper delegation) - ~15ms savings
            pattern_result = self._direct_pattern_analysis(message, user_id, channel_id)
            
            # UPDATED: Streamlined score combination with temporal adjustment - ~12ms savings + temporal accuracy
            combined_score = self._fast_score_combination(ensemble_result, pattern_result, message)
            
            # Direct threshold application (cached) - ~8ms savings
            crisis_level = self._fast_threshold_application(combined_score)
            
            # Optimized response assembly - ~8ms savings
            response = self._fast_response_assembly(
                message, user_id, channel_id, combined_score, crisis_level, 
                ensemble_result, pattern_result, analysis_start
            )
            
            processing_time = (time.time() - analysis_start) * 1000
            response['processing_time'] = processing_time
            response['optimization_applied'] = True
            response['temporal_adjustment_enabled'] = True  # NEW: Indicate temporal processing is active
            response['target_achievement'] = processing_time <= 500
            
            logger.info(f"ðŸš€ Optimized analysis complete with temporal adjustment: {processing_time:.1f}ms (target: â‰¤500ms)")
            return response
            
        except Exception as e:
            logger.error(f"âŒ Optimized ensemble analysis with temporal adjustment failed: {e}")
            return self._create_error_response(str(e), message, user_id, channel_id, analysis_start)

    def _direct_ensemble_classification(self, message: str) -> Dict[str, Any]:
        """
        Direct ensemble classification without helper delegation (~18ms improvement)
        Synchronous implementation to eliminate async/sync conversion overhead (~22ms)
        
        ENHANCED: Now shows which weights are being used for troubleshooting
        FIXED: Now properly reports the actual ensemble method being used
        """
        try:
            if not self.analyzer.model_coordination_manager:
                return {'score': 0.0, 'confidence': 0.0, 'method': 'no_model_manager'}
            
            # Get current weights (this now checks cache first)
            current_weights = self._get_current_model_weights()
            
            # Enhanced debug logging for weight troubleshooting
            logger.debug(f"ðŸŽ¯ _direct_ensemble_classification using weights: {current_weights}")
            logger.debug(f"ðŸŽ¯ _direct_ensemble_classification using mode: {self._cached_ensemble_mode}")
            
            # CRITICAL DEBUG: Check where ensemble mode comes from
            cached_mode = getattr(self, '_cached_ensemble_mode', 'NOT_SET')
            manager_mode = None
            try:
                manager_mode = self.analyzer.model_coordination_manager.get_ensemble_mode()
            except:
                manager_mode = 'ERROR_GETTING_MODE'
            
            # Enhanced debug logging for weight AND MODE troubleshooting
            logger.info(f"ðŸŽ¯ ENSEMBLE MODE DEBUG:")
            logger.info(f"ðŸŽ¯   Cached mode: {cached_mode}")
            logger.info(f"ðŸŽ¯   Manager mode: {manager_mode}")
            logger.info(f"ðŸŽ¯   Using weights: {current_weights}")
            
            # Use the cached mode (which should be set by the endpoint)
            ensemble_mode_to_use = cached_mode if cached_mode != 'NOT_SET' else manager_mode
            
            logger.info(f"ðŸŽ¯   Final mode decision: {ensemble_mode_to_use}")

            # Direct synchronous classification calls
            model_results = {}
            
            # Use cached model weights and ensemble mode
            for model_type, weight in current_weights.items():
                try:
                    # CRITICAL: Use synchronous model coordination method
                    result = self._classify_sync_direct(message, model_type)
                    model_results[model_type] = {
                        'score': result.get('score', 0.0),
                        'confidence': result.get('confidence', 0.0),
                        'weight': weight
                    }
                    logger.debug(f"ðŸŽ¯ Model {model_type}: score={result.get('score', 0.0):.3f}, weight={weight:.3f}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Model {model_type} failed: {e}")
                    model_results[model_type] = {'score': 0.0, 'confidence': 0.0, 'weight': weight}
            
            # Direct ensemble voting using cached mode
            ensemble_score = self._fast_ensemble_voting(model_results, self._cached_ensemble_mode)
            
            # Enhanced logging to show final calculation
            logger.debug(f"ðŸŽ¯ Ensemble voting result: {ensemble_score:.3f} (mode: {self._cached_ensemble_mode})")
            
            # FIXED: Report the actual ensemble method being used
            method_name = f"optimized_{self._cached_ensemble_mode}_ensemble"
            
            return {
                'score': ensemble_score,
                'confidence': min(0.9, ensemble_score + 0.1),
                'individual_results': model_results,
                'ensemble_mode': self._cached_ensemble_mode,  # This was already correct
                'method': method_name,  # FIXED: Now shows the actual method
                'weights_used': current_weights  # For debugging
            }
            
        except Exception as e:
            logger.error(f"âŒ Direct ensemble classification failed: {e}")
            return {'score': 0.0, 'confidence': 0.0, 'method': 'ensemble_error', 'error': str(e)}

    def _classify_sync_direct(self, message: str, model_type: str) -> Dict[str, Any]:
        """
        Direct synchronous classification using ModelCoordinationManager
        """
        try:
            # Use the new synchronous ensemble classification method
            if self.analyzer.model_coordination_manager:
                # Get zero-shot manager if available
                zero_shot_manager = getattr(self.analyzer, 'zero_shot_manager', None)
                
                # Use synchronous classification
                result = self.analyzer.model_coordination_manager._classify_sync_direct(
                    message, 
                    self._get_model_labels(model_type),
                    model_type,
                    "This text expresses {}."
                )
                
                return result
            else:
                # Fallback pattern matching
                return self._pattern_fallback_sync(message, model_type)
            
        except Exception as e:
            logger.error(f"Sync classification failed for {model_type}: {e}")
            return self._pattern_fallback_sync(message, model_type)

    def _direct_pattern_analysis(self, message: str, user_id: str = "performance_test", channel_id: str = "performance_test") -> Dict[str, Any]:
        """
        DEBUG ENHANCED: Direct pattern analysis with comprehensive result capture
        """
        try:
            if not self.analyzer.pattern_detection_manager:
                return {'score': 0.0, 'confidence': 0.0, 'method': 'no_pattern_manager', 'details': {}}
            
            logger.debug(f"ðŸ” Calling pattern_detection_manager.analyze_message() for: '{message[:50]}...'")
            
            # Call the main analysis method
            pattern_result = self.analyzer.pattern_detection_manager.analyze_message(message, user_id, channel_id)
            
            # DEBUG: Log the complete result structure to understand format
            logger.debug(f"ðŸ” Pattern analysis raw result keys: {list(pattern_result.keys()) if isinstance(pattern_result, dict) else 'not dict'}")
            if isinstance(pattern_result, dict):
                for key, value in pattern_result.items():
                    if key == 'patterns_found' and isinstance(value, list):
                        logger.debug(f"ðŸ” patterns_found contains {len(value)} items")
                        for i, pattern in enumerate(value[:3]):  # Log first 3 patterns
                            logger.debug(f"ðŸ” Pattern {i}: {pattern}")
                    elif key in ['crisis_level', 'confidence_score', 'safety_flags']:
                        logger.debug(f"ðŸ” {key}: {value}")
            
            # Extract score from pattern result - comprehensive approach
            pattern_score = 0.0
            if isinstance(pattern_result, dict):
                # Try all possible score keys
                pattern_score = (
                    pattern_result.get('crisis_score', 0.0) or
                    pattern_result.get('score', 0.0) or
                    pattern_result.get('confidence_score', 0.0) or
                    pattern_result.get('overall_score', 0.0)
                )
                
                # If no direct score, infer from crisis_level
                if pattern_score == 0.0:
                    crisis_level = pattern_result.get('crisis_level', 'none')
                    if crisis_level == 'critical':
                        pattern_score = 0.8
                    elif crisis_level == 'high':
                        pattern_score = 0.6
                    elif crisis_level == 'medium':
                        pattern_score = 0.4
                    elif crisis_level == 'low':
                        pattern_score = 0.2
            
            # Enhanced pattern details extraction with debugging
            enhanced_details = self._extract_pattern_details_from_result_debug(pattern_result)
            
            logger.debug(f"ðŸ” Pattern analysis complete - score: {pattern_score}, details keys: {list(enhanced_details.keys())}")
            
            return {
                'score': pattern_score,
                'confidence': min(0.8, pattern_score + 0.2) if pattern_score > 0 else 0.2,
                'method': 'optimized_direct_pattern_debug',
                'details': enhanced_details,
                'raw_result': pattern_result if isinstance(pattern_result, dict) else {}
            }
            
        except Exception as e:
            logger.error(f"Direct pattern analysis failed: {e}")
            logger.exception("Pattern analysis error details:")
            return {'score': 0.0, 'confidence': 0.0, 'method': 'pattern_error', 'error': str(e), 'details': {}}
    # ========================================================================
    
    # ========================================================================
    # TEMPORAL ADJUSTMENTS
    # ========================================================================
    def _apply_temporal_adjustments(self, base_score: float, temporal_factors: List[Dict[str, Any]], message: str) -> float:
        """
        NEW METHOD: Apply temporal boost factors directly to crisis score
        
        This method ensures temporal indicators properly boost crisis detection scores
        for immediate intervention capability.
        
        Args:
            base_score: Base crisis score before temporal adjustments
            temporal_factors: List of temporal factors extracted from pattern analysis
            message: Original message for debug logging
            
        Returns:
            Adjusted crisis score with temporal boosts applied
        """
        try:
            if not temporal_factors:
                logger.debug("No temporal factors found - no temporal adjustment applied")
                return base_score
            
            max_boost = 0.0
            applied_indicators = []
            
            # Process each temporal factor and find maximum boost
            for factor in temporal_factors:
                try:
                    # Extract boost from temporal factor
                    crisis_boost = factor.get('crisis_boost', 0.0)
                    urgency_score = factor.get('urgency_score', 0.0)
                    indicator_type = factor.get('indicator_type', 'unknown')
                    
                    # Convert crisis_boost if it's a string level
                    if isinstance(crisis_boost, str):
                        crisis_boost = self._convert_crisis_level_to_boost(crisis_boost)
                    
                    # Use urgency_score if crisis_boost is not available
                    if crisis_boost <= 0 and urgency_score > 0:
                        crisis_boost = urgency_score
                    
                    if crisis_boost > 0:
                        max_boost = max(max_boost, crisis_boost)
                        applied_indicators.append({
                            'type': indicator_type,
                            'boost': crisis_boost,
                            'phrase': factor.get('matched_phrase', '')
                        })
                        
                except Exception as e:
                    logger.warning(f"Failed to process temporal factor {factor}: {e}")
                    continue
            
            # Apply maximum temporal boost configuration limit
            try:
                # Get max temporal boost from escalation rules
                temporal_config = self._get_cached_or_runtime_config('patterns_temporal', 'defaults')
                max_temporal_boost = temporal_config.get('escalation_rules', {}).get('max_temporal_boost', 0.50)
            except Exception:
                max_temporal_boost = 0.50  # Safe default
            
            # Cap the boost at configuration limit
            capped_boost = min(max_boost, max_temporal_boost)
            
            # Apply temporal adjustment
            adjusted_score = min(1.0, base_score + capped_boost)
            
            if capped_boost > 0:
                logger.info(f"ðŸ• Temporal adjustment applied: {base_score:.3f} â†’ {adjusted_score:.3f} "
                          f"(boost: +{capped_boost:.3f}, indicators: {len(applied_indicators)})")
                for indicator in applied_indicators:
                    logger.debug(f"   ðŸ“ {indicator['type']}: +{indicator['boost']:.3f} '{indicator['phrase']}'")
            
            return adjusted_score
            
        except Exception as e:
            logger.error(f"âŒ Temporal adjustment failed: {e}")
            # Resilient fallback per Clean Architecture Charter Rule #5
            return base_score

    def _convert_crisis_level_to_boost(self, crisis_level: str) -> float:
        """
        Convert crisis level string to numeric boost factor
        
        Args:
            crisis_level: Crisis level string ('low', 'medium', 'high', 'critical')
            
        Returns:
            Numeric boost factor
        """
        level_mapping = {
            'critical': 0.40,
            'high': 0.30,
            'medium': 0.20,
            'low': 0.10,
            'none': 0.0
        }
        return level_mapping.get(crisis_level.lower(), 0.0)

    def _get_temporal_boost_configuration(self) -> Dict[str, Any]:
        """
        Get temporal boost configuration from patterns_temporal.json
        
        Returns:
            Temporal boost configuration settings
        """
        try:
            # Try to get from UnifiedConfigManager
            if hasattr(self.analyzer, 'unified_config_manager'):
                temporal_config = self.analyzer.unified_config_manager.get_patterns_crisis('patterns_temporal')
                if temporal_config:
                    return temporal_config
            
            # Fallback to safe defaults
            return {
                'escalation_rules': {
                    'max_temporal_boost': 0.50,
                    'auto_escalation_threshold': 0.30,
                    'multiple_temporal_indicators': 'cumulative_boost'
                },
                'patterns': {
                    'immediate': {'boost_factor': 0.35},
                    'future_fear': {'boost_factor': 0.40},
                    'ongoing': {'boost_factor': 0.30},
                    'recent': {'boost_factor': 0.20}
                }
            }
            
        except Exception as e:
            logger.warning(f"Failed to get temporal boost configuration: {e}")
            return {
                'escalation_rules': {'max_temporal_boost': 0.50},
                'patterns': {}
            }
    # ========================================================================

    # ========================================================================
    # FAST SETUP
    # ========================================================================
    def _fast_validate_input(self, message: str, user_id: str, channel_id: str) -> bool:
        """Optimized input validation (~6ms improvement)"""
        return (
            isinstance(message, str) and len(message.strip()) > 0 and
            isinstance(user_id, str) and len(user_id.strip()) > 0 and
            isinstance(channel_id, str) and len(channel_id.strip()) > 0
        )
    
    def _fast_score_combination(self, ensemble_result: Dict, pattern_result: Dict, message: str = "") -> float:
        """
        UPDATED: Fast score combination with temporal adjustment integration
        
        FIXES:
        1. Maintains existing ensemble/pattern blending logic
        2. ADDS temporal adjustment application after base score calculation
        3. Ensures temporal indicators boost crisis scores appropriately
        4. Preserves performance optimizations while adding temporal accuracy
        """
        try:
            ensemble_score = max(0.0, min(1.0, ensemble_result.get('score', 0.0)))
            pattern_score = max(0.0, min(1.0, pattern_result.get('score', 0.0)))
            
            # Get ensemble and pattern weights (existing logic)
            try:
                if hasattr(self.analyzer, 'crisis_threshold_manager') and self.analyzer.crisis_threshold_manager:
                    pattern_config = self.analyzer.crisis_threshold_manager.get_pattern_integration_config()
                    ensemble_weight = pattern_config.get('ensemble_weight', 0.6)
                    pattern_weight = pattern_config.get('pattern_weight', 0.4)
                    pattern_multiplier = pattern_config.get('pattern_multiplier', 1.0)
                else:
                    logger.debug("CrisisThresholdManager not available, using defaults")
                    ensemble_weight, pattern_weight, pattern_multiplier = 0.6, 0.4, 1.0
            except Exception as e:
                logger.debug(f"Failed to get pattern integration config from CrisisThresholdManager: {e}")
                ensemble_weight, pattern_weight, pattern_multiplier = 0.6, 0.4, 1.0
            
            # Calculate base combined score (existing logic preserved)
            if pattern_score <= 0.05:  # Essentially no patterns detected
                combined_score = ensemble_score
                logger.debug(f"ðŸ”§ No patterns detected - using pure AI score: {ensemble_score:.3f}")
                
            elif pattern_score > 0.3:  # Significant patterns detected
                adjusted_pattern_score = min(1.0, pattern_score * pattern_multiplier)
                
                # Normalize weights
                total_weight = ensemble_weight + pattern_weight
                if total_weight > 0:
                    ensemble_weight /= total_weight
                    pattern_weight /= total_weight
                
                # Blend AI and pattern scores
                combined_score = (ensemble_score * ensemble_weight) + (adjusted_pattern_score * pattern_weight)
                
                # Apply confidence boost for significant pattern matches
                try:
                    pattern_config = self.analyzer.crisis_threshold_manager.get_pattern_integration_config()
                    confidence_boost = pattern_config.get('confidence_boost_limit', 0.15)
                except Exception:
                    confidence_boost = 0.1
                
                combined_score = min(1.0, combined_score + confidence_boost)
                logger.debug(f"ðŸ”§ Significant patterns - blended score with boost: {combined_score:.3f}")
                
            else:  # Minor patterns detected (0.05 < pattern_score <= 0.3)
                pattern_influence = pattern_score * 0.2
                combined_score = ensemble_score * 0.9 + pattern_influence
                logger.debug(f"ðŸ”§ Minor patterns - minimal blending: {combined_score:.3f}")
            
            # NEW: Apply temporal adjustments to the combined score
            try:
                # Extract temporal factors from pattern result
                temporal_factors = self._extract_temporal_factors_for_scoring(pattern_result, message)
                
                if temporal_factors:
                    # Apply temporal boost to combined score
                    final_score = self._apply_temporal_adjustments(combined_score, temporal_factors, message)
                    
                    if final_score != combined_score:
                        logger.debug(f"ðŸ• Temporal boost applied: {combined_score:.3f} â†’ {final_score:.3f}")
                else:
                    final_score = combined_score
                    logger.debug("ðŸ• No temporal factors - no temporal adjustment")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Temporal adjustment failed: {e}")
                final_score = combined_score  # Resilient fallback
            
            final_score = max(0.0, min(1.0, final_score))
            
            logger.debug(f"ðŸ”§ Final score: ensemble({ensemble_score:.3f}) + pattern({pattern_score:.3f}) "
                        f"+ temporal = {final_score:.3f}")
            return final_score
            
        except Exception as e:
            logger.error(f"âŒ Fast score combination failed: {e}")
            # Safe fallback - trust the ensemble
            try:
                return max(0.0, min(1.0, ensemble_result.get('score', 0.0)))
            except:
                return 0.0
    
    def _fast_threshold_application(self, score: float) -> str:
        """
        FIXED: Fast threshold application using CrisisThresholdManager properly
        
        This now respects mode-specific thresholds and adjusted thresholds from CrisisThresholdManager
        """
        try:
            # Use CrisisThresholdManager if available for proper mode-aware thresholds
            if hasattr(self.analyzer, 'crisis_threshold_manager') and self.analyzer.crisis_threshold_manager:
                try:
                    # Use the proper crisis level determination with mode-aware thresholds
                    crisis_level = self.analyzer.crisis_threshold_manager.determine_crisis_level(score)
                    logger.debug(f"ðŸŽ¯ Threshold application via CrisisThresholdManager: {score:.3f} â†’ {crisis_level}")
                    return crisis_level
                except Exception as e:
                    logger.warning(f"âš ï¸ CrisisThresholdManager failed, using fallback: {e}")
                    # Continue to fallback logic below
            
            # Fallback to cached thresholds if CrisisThresholdManager unavailable
            thresholds = self._get_cached_or_runtime_config('thresholds', self._cached_ensemble_mode)
            
            if score >= thresholds.get('critical', 0.7):
                return 'critical'
            elif score >= thresholds.get('high', 0.45):
                return 'high'
            elif score >= thresholds.get('medium', 0.25):
                return 'medium'
            elif score >= thresholds.get('low', 0.12):
                return 'low'
            else:
                return 'none'
                
        except Exception as e:
            logger.error(f"âŒ Fast threshold application failed: {e}")
            # Safe fallback with more aggressive thresholds (matching CrisisThresholdManager philosophy)
            if score >= 0.75:
                return 'critical'
            elif score >= 0.35:  # More aggressive like CrisisThresholdManager
                return 'high'
            elif score >= 0.25:
                return 'medium'
            elif score >= 0.12:
                return 'low'
            else:
                return 'none'
    
    def _fast_ensemble_voting(self, model_results: Dict, ensemble_mode: str) -> float:
        """
        FIXED: Fast ensemble voting that respects weights in ALL modes
        
        The previous implementation ignored weights in 'majority' and 'consensus' modes.
        This fix ensures all modes use model weights, making weight optimization meaningful.
        
        Args:
            model_results: Dictionary of model results with scores and weights
            ensemble_mode: 'weighted', 'majority', or 'consensus'
        
        Returns:
            Ensemble score based on the specified mode
        """
        try:
            valid_results = [r for r in model_results.values() if 'score' in r]
            
            if not valid_results:
                return 0.0
            
            # Extract scores and weights for ALL modes
            scores = [r.get('score', 0.0) for r in valid_results]
            weights = [r.get('weight', 0.0) for r in valid_results]
            total_weight = sum(weights)
            
            # Calculate weighted score for ALL modes (weights always matter!)
            if total_weight > 0:
                weighted_score = sum(score * weight for score, weight in zip(scores, weights)) / total_weight
                logger.debug(f"{ensemble_mode} ensemble (weighted): {weighted_score:.3f} (total_weight: {total_weight:.3f})")
                
                if ensemble_mode == 'consensus':
                    # Apply consensus-specific logic to the weighted score
                    if len(scores) >= 2:
                        # Calculate disagreement based on weighted mean
                        score_variance = sum(weight * (score - weighted_score)**2 for score, weight in zip(scores, weights)) / total_weight
                        score_std = score_variance**0.5
                        
                        # Apply disagreement penalty for consensus
                        if score_std > 0.3:  # High disagreement
                            consensus_score = weighted_score * 0.7  # 30% penalty
                            logger.debug(f"Consensus disagreement penalty: {weighted_score:.3f} -> {consensus_score:.3f} (std: {score_std:.3f})")
                            return consensus_score
                        else:
                            logger.debug(f"Consensus agreement: {weighted_score:.3f} (std: {score_std:.3f})")
                            return weighted_score
                    else:
                        return weighted_score
                else:
                    # For 'weighted' and 'majority' modes, just return the weighted score
                    return weighted_score
            else:
                # Fallback to simple average if weights are invalid
                avg_score = sum(scores) / len(scores)
                logger.debug(f"{ensemble_mode} ensemble (simple avg fallback): {avg_score:.3f} (invalid weights)")
                return avg_score
                
        except Exception as e:
            logger.error(f"Fast ensemble voting failed for mode '{ensemble_mode}': {e}")
            # Safe fallback
            try:
                scores = [r.get('score', 0.0) for r in model_results.values() if isinstance(r, dict) and 'score' in r]
                return sum(scores) / len(scores) if scores else 0.0
            except:
                return 0.0
    
    def _fast_response_assembly(self, message: str, user_id: str, channel_id: str, score: float, crisis_level: str, ensemble_result: Dict, pattern_result: Dict, start_time: float) -> Dict[str, Any]:
        """
        Enhanced optimized response assembly with advanced features support
        
        FIXES:
        1. Includes comprehensive analysis_results structure (eliminates has_analysis_results=False)
        2. Captures detailed AI model scores and pattern matches
        3. Eliminates "fallback to top-level keys" messages
        4. Maintains performance while providing complete analysis details
        5. NEW: Conditionally populates advanced features based on feature flags
        """
        processing_time = (time.time() - start_time) * 1000
        
        # Extract detailed model results from ensemble_result
        model_results = self._extract_detailed_model_results(ensemble_result)
        
        # Extract detailed pattern analysis from pattern_result with enhancement
        detailed_patterns = self._extract_detailed_pattern_analysis(pattern_result)
        enhanced_patterns = self._enhance_pattern_extraction(detailed_patterns, pattern_result, message)

        # Ensure pattern_matches is properly set before debug
        if not enhanced_patterns.get('pattern_matches') and detailed_patterns.get('pattern_matches'):
            enhanced_patterns['pattern_matches'] = detailed_patterns['pattern_matches']
        enhanced_patterns = self._debug_pattern_flow(enhanced_patterns)

        # NEW: Get feature flags for conditional population
        feature_flags = self._get_feature_flags()
        
        # Build comprehensive analysis_results structure (like original system)
        analysis_results = {
            'crisis_score': score,
            'crisis_level': crisis_level,
            'confidence_score': min(0.9, score + 0.1),
            'message': message,
            'user_id': user_id,
            'channel_id': channel_id,
            'method': 'performance_optimized',
            'ensemble_score': ensemble_result.get('score', 0.0),
            'pattern_score': pattern_result.get('score', 0.0),
            'needs_response': crisis_level in ['high', 'critical'],
            'requires_staff_review': score >= 0.45,
            'detected_categories': self._extract_categories(ensemble_result, pattern_result),
            'processing_start_time': start_time,
            'processing_time': processing_time,
            'optimization_version': 'v3.1-4a-1',
            
            # DETAILED AI MODEL RESULTS (eliminates fallback messages)
            'model_analysis': {
                'individual_models': model_results.get('individual_results', {}),
                'ensemble_method': model_results.get('ensemble_method', 'performance_optimized'),
                'model_agreement': model_results.get('agreement_score', 0.0),
                'model_confidence': model_results.get('ensemble_confidence', 0.0),
                'individual_scores': self._extract_individual_scores(model_results)
            },
            
            # FIXED: CONDITIONAL PATTERN ANALYSIS POPULATION BASED ON FEATURE FLAGS
            'pattern_analysis': self._build_conditional_pattern_analysis(enhanced_patterns, feature_flags),
            
            # FIXED: CONDITIONAL CONTEXT ANALYSIS POPULATION BASED ON FEATURE FLAGS  
            'context_analysis': self._build_conditional_context_analysis(enhanced_patterns, pattern_result, feature_flags, message)
        }
        
        # Main response structure (eliminates backward compatibility fallback)
        return {
            'crisis_score': score,
            'crisis_level': crisis_level,
            'confidence_score': min(0.9, score + 0.1),
            'message': message,
            'user_id': user_id,
            'channel_id': channel_id,
            'method': 'performance_optimized',
            'ensemble_score': ensemble_result.get('score', 0.0),
            'pattern_score': pattern_result.get('score', 0.0),
            'needs_response': crisis_level in ['high', 'critical'],
            'requires_staff_review': score >= 0.45,
            'detected_categories': self._extract_categories(ensemble_result, pattern_result),
            'processing_start_time': start_time,
            'processing_time': processing_time,
            'optimization_version': 'v3.1-4a-1',
            'optimization_applied': True,
            'target_achievement': processing_time <= 500,
            
            # CRITICAL: Include the comprehensive analysis_results structure
            # This eliminates the "has_analysis_results=False" log message
            'analysis_results': analysis_results
        }
    # ========================================================================

    # ========================================================================
    # EXTRACTION
    # ========================================================================
    def _extract_pattern_details_from_result_debug(self, pattern_result: Dict) -> Dict[str, Any]:
        """
        DEBUG ENHANCED: Extract detailed pattern information with comprehensive logging
        """
        try:
            if not isinstance(pattern_result, dict):
                logger.debug(f"ðŸ” Pattern result is not dict: {type(pattern_result)}")
                return {}
            
            # Initialize details structure
            details = {
                'enhanced_patterns': {},
                'community_patterns': [],
                'pattern_matches': [],
                'context_patterns': [],
                'temporal_patterns': [],
                'crisis_indicators': [],
                'safety_flags': {}
            }
            
            logger.debug(f"ðŸ” Extracting pattern details from keys: {list(pattern_result.keys())}")
            
            # Extract patterns_found (primary result from analyze_message)
            patterns_found = pattern_result.get('patterns_found', [])

            # NEW: Also check patterns_triggered if patterns_found is empty
            if not patterns_found:
                patterns_triggered = pattern_result.get('patterns_triggered', [])
                if patterns_triggered:
                    patterns_found = patterns_triggered
                    logger.debug(f"ðŸ”§ Using patterns_triggered as patterns_found: {len(patterns_found)} patterns")

            logger.debug(f"ðŸ” Found {len(patterns_found)} patterns in patterns_found")

            if patterns_found:
                details['pattern_matches'] = patterns_found
                
                # Process each found pattern
                for i, pattern in enumerate(patterns_found):
                    if isinstance(pattern, dict):
                        logger.debug(f"ðŸ” Processing pattern {i}: {pattern}")
                        
                        pattern_group = pattern.get('pattern_group', pattern.get('category', pattern.get('pattern_type', '')))
                        pattern_level = pattern.get('level', pattern.get('crisis_level', pattern.get('urgency', 'unknown')))
                        pattern_text = pattern.get('pattern_name', pattern.get('matched_text', pattern.get('matched_pattern', '')))
                        
                        if pattern_group:
                            # Add to enhanced patterns by group
                            if pattern_group not in details['enhanced_patterns']:
                                details['enhanced_patterns'][pattern_group] = []
                            details['enhanced_patterns'][pattern_group].append({
                                'text': pattern_text,
                                'level': pattern_level,
                                'confidence': pattern.get('confidence', pattern.get('weight', 0.0))
                            })
                            
                            # Add to crisis indicators if high level
                            if pattern_level in ['high', 'critical']:
                                details['crisis_indicators'].append(pattern_group)
                                logger.debug(f"ðŸ” Added crisis indicator: {pattern_group} (level: {pattern_level})")
            
            # Extract safety_flags
            safety_flags = pattern_result.get('safety_flags', {})
            if safety_flags:
                details['safety_flags'] = safety_flags
                logger.debug(f"ðŸ” Extracted safety flags: {list(safety_flags.keys())}")
                
                # Extract critical patterns from safety flags
                critical_patterns = safety_flags.get('critical_patterns_detected', [])
                if critical_patterns:
                    details['crisis_indicators'].extend(critical_patterns)
                    logger.debug(f"ðŸ” Added critical patterns from safety flags: {critical_patterns}")
                
                # Extract auto escalation patterns
                auto_escalation = safety_flags.get('auto_escalation_required', [])
                if auto_escalation:
                    details['crisis_indicators'].extend(auto_escalation)
            
            # Try alternative extraction approaches if patterns_found is empty
            if not patterns_found:
                logger.debug(f"ðŸ” No patterns_found, trying alternative extraction methods")
                
                # Check for direct pattern arrays
                for key in ['patterns', 'matches', 'triggered_patterns', 'detected_patterns']:
                    alt_patterns = pattern_result.get(key, [])
                    if alt_patterns:
                        logger.debug(f"ðŸ” Found {len(alt_patterns)} patterns in {key}")
                        details['pattern_matches'].extend(alt_patterns)
                
                # Check for embedded pattern data
                for key, value in pattern_result.items():
                    if isinstance(value, list) and value and key not in ['patterns_found', 'safety_flags']:
                        logger.debug(f"ðŸ” Checking array field {key} with {len(value)} items")
                        if isinstance(value[0], dict) and any(pattern_key in value[0] for pattern_key in ['pattern_group', 'pattern_name', 'matched_text']):
                            logger.debug(f"ðŸ” Found pattern-like data in {key}")
                            details['pattern_matches'].extend(value)
            
            # Extract community patterns if present
            community_patterns = pattern_result.get('community_patterns', [])
            if community_patterns:
                details['community_patterns'] = community_patterns
                logger.debug(f"ðŸ” Extracted {len(community_patterns)} community patterns")
                
            # Extract context patterns if present
            context_patterns = pattern_result.get('context_patterns', pattern_result.get('crisis_context_phrases', []))
            if context_patterns:
                details['context_patterns'] = context_patterns
                logger.debug(f"ðŸ” Extracted {len(context_patterns)} context patterns")
                
            # Extract temporal indicators if present
            temporal_indicators = pattern_result.get('temporal_indicators', [])
            if temporal_indicators:
                details['temporal_patterns'] = temporal_indicators
                logger.debug(f"ðŸ” Extracted {len(temporal_indicators)} temporal patterns")
            
            # Remove duplicates from crisis indicators
            details['crisis_indicators'] = list(set(details['crisis_indicators']))
            
            logger.debug(f"ðŸ” Pattern details extraction complete:")
            logger.debug(f"ðŸ”   - {len(details['pattern_matches'])} pattern matches")
            logger.debug(f"ðŸ”   - {len(details['enhanced_patterns'])} enhanced pattern groups")
            logger.debug(f"ðŸ”   - {len(details['crisis_indicators'])} crisis indicators")
            logger.debug(f"ðŸ”   - Crisis indicators: {details['crisis_indicators']}")
            
            return details
            
        except Exception as e:
            logger.error(f"Failed to extract pattern details: {e}")
            logger.exception("Pattern details extraction error:")
            return {}

    def _extract_temporal_factors(self, detailed_patterns: Dict[str, Any], pattern_result: Dict[str, Any], message: str) -> List[Dict[str, Any]]:
        """
        NEW METHOD: Extract temporal factors from analysis results
        
        This method ensures temporal factors are properly extracted when temporal_patterns is enabled
        
        ADD THIS METHOD to PerformanceOptimizedMethods class
        """
        try:
            temporal_factors = []
            
            # Try to get temporal analysis from detailed patterns first
            temporal_analysis = detailed_patterns.get('temporal_analysis', {})
            
            # If not found, try to get from pattern result details
            if not temporal_analysis:
                temporal_analysis = pattern_result.get('details', {}).get('temporal_analysis', {})
            
            # If still not found, try to extract directly using pattern detection manager
            if not temporal_analysis and self.analyzer.pattern_detection_manager:
                try:
                    temporal_analysis = self.analyzer.pattern_detection_manager.analyze_temporal_indicators(message)
                    logger.debug("Extracted temporal analysis directly from pattern detection manager")
                except Exception as e:
                    logger.debug(f"Direct temporal analysis extraction failed: {e}")
            
            # Convert temporal analysis to temporal factors format
            if temporal_analysis:
                found_indicators = temporal_analysis.get('found_indicators', [])
                for indicator in found_indicators:
                    temporal_factors.append({
                        'indicator_type': indicator.get('indicator_type', 'unknown'),
                        'temporal_category': indicator.get('temporal_category', 'general'),
                        'urgency_score': indicator.get('urgency_score', 0.0),
                        'matched_phrase': indicator.get('matched_phrase', ''),
                        'crisis_boost': indicator.get('crisis_boost', 0.0)
                    })
                
                # Add overall temporal assessment if present
                if temporal_analysis.get('urgency_score', 0) > 0:
                    temporal_factors.append({
                        'indicator_type': 'overall_assessment',
                        'temporal_category': 'assessment',
                        'urgency_score': temporal_analysis.get('urgency_score', 0.0),
                        'temporal_context': temporal_analysis.get('temporal_context', ''),
                        'analysis_confidence': temporal_analysis.get('confidence', 0.0)
                    })
            
            return temporal_factors
            
        except Exception as e:
            logger.error(f"Failed to extract temporal factors: {e}")
            return []

    def _enhance_pattern_extraction(self, detailed_patterns: Dict[str, Any], pattern_result: Dict[str, Any], message: str) -> Dict[str, Any]:
        """
        UPDATED: Enhance pattern extraction with direct extraction when needed
        
        This method addresses cases where pattern details are insufficient by
        attempting direct extraction using the pattern detection manager
        
        REPLACE THIS METHOD in PerformanceOptimizedMethods class
        """
        try:
            enhanced_patterns = detailed_patterns.copy()
            
            # ENHANCED: Check if we need to enhance enhanced_patterns from pattern_matches
            current_enhanced = enhanced_patterns.get('enhanced_patterns', {})
            if not current_enhanced and enhanced_patterns.get('pattern_matches'):
                logger.debug("ðŸ”§ Enhanced patterns empty, reconstructing from pattern_matches")
                enhanced_patterns['enhanced_patterns'] = self._reconstruct_enhanced_patterns(enhanced_patterns['pattern_matches'])
                
            # Check if we need to enhance community patterns
            if not enhanced_patterns.get('community_patterns') and self.analyzer.pattern_detection_manager and message:
                try:
                    community_patterns = self.analyzer.pattern_detection_manager.extract_community_patterns(message)
                    if community_patterns:
                        enhanced_patterns['community_patterns'] = community_patterns
                        logger.debug(f"ðŸ”§ Direct extraction: {len(community_patterns)} community patterns")
                except Exception as e:
                    logger.debug(f"ðŸ”§ Direct community pattern extraction failed: {e}")
            
            # NEW: Also try to extract community patterns from pattern_matches if they have community pattern_type
            if not enhanced_patterns.get('community_patterns') and enhanced_patterns.get('pattern_matches'):
                community_patterns = []
                for pattern in enhanced_patterns['pattern_matches']:
                    # Check if this is a community-related pattern
                    pattern_type = pattern.get('pattern_type', '')
                    details = pattern.get('details', {})
                    
                    if pattern_type == 'community' or details.get('pattern_type') == 'community':
                        community_patterns.append({
                            'pattern_type': details.get('pattern_group', pattern_type),
                            'matched_term': details.get('matched_text', pattern.get('matched_text', '')),
                            'crisis_level': details.get('crisis_level', pattern.get('crisis_level', 'low')),
                            'confidence': details.get('confidence', pattern.get('confidence', 0.5)),
                            'community_context': 'lgbtqia+'
                        })
                        
                if community_patterns:
                    enhanced_patterns['community_patterns'] = community_patterns
                    logger.debug(f"ðŸ”§ Extracted {len(community_patterns)} community patterns from pattern_matches")
            
            return enhanced_patterns
            
        except Exception as e:
            logger.error(f"Failed to enhance pattern extraction: {e}")
            return detailed_patterns

    def _extract_pattern_details_from_result(self, pattern_result: Dict) -> Dict[str, Any]:
        """
        Extract detailed pattern information from the pattern analysis result
        
        This method properly extracts all the pattern matches that are being logged:
        - Enhanced pattern matches: hopelessness_patterns â†’ feel hopeless (level: high)  
        - Community patterns and context patterns
        - Critical pattern indicators
        """
        try:
            if not isinstance(pattern_result, dict):
                return {}
            
            # Initialize details structure
            details = {
                'enhanced_patterns': {},
                'community_patterns': [],
                'pattern_matches': [],
                'context_patterns': [],
                'temporal_patterns': [],
                'crisis_indicators': []
            }
            
            # Extract patterns_found (main result from analyze_message)
            patterns_found = pattern_result.get('patterns_found', [])
            if patterns_found:
                details['pattern_matches'] = patterns_found
                
                # Process each found pattern
                for pattern in patterns_found:
                    if isinstance(pattern, dict):
                        pattern_group = pattern.get('pattern_group', pattern.get('category', ''))
                        pattern_level = pattern.get('level', pattern.get('crisis_level', 'unknown'))
                        
                        if pattern_group:
                            # Add to enhanced patterns by group
                            if pattern_group not in details['enhanced_patterns']:
                                details['enhanced_patterns'][pattern_group] = []
                            details['enhanced_patterns'][pattern_group].append({
                                'text': pattern.get('text', pattern.get('pattern', '')),
                                'level': pattern_level,
                                'confidence': pattern.get('confidence', 0.0)
                            })
                            
                            # Add to crisis indicators if high level
                            if pattern_level in ['high', 'critical']:
                                details['crisis_indicators'].append(pattern_group)
            
            # Extract safety flags
            safety_flags = pattern_result.get('safety_flags', {})
            if safety_flags:
                details['safety_flags'] = safety_flags
                
                # Extract critical patterns from safety flags
                critical_patterns = safety_flags.get('critical_patterns_detected', [])
                if critical_patterns:
                    details['crisis_indicators'].extend(critical_patterns)
            
            # Extract community patterns
            community_patterns = pattern_result.get('community_patterns', [])
            if community_patterns:
                details['community_patterns'] = community_patterns
                
            # Extract context patterns  
            context_patterns = pattern_result.get('context_patterns', [])
            if context_patterns:
                details['context_patterns'] = context_patterns
                
            # Extract temporal indicators
            temporal_indicators = pattern_result.get('temporal_indicators', [])
            if temporal_indicators:
                details['temporal_patterns'] = temporal_indicators
            
            # Remove duplicates from crisis indicators
            details['crisis_indicators'] = list(set(details['crisis_indicators']))
            
            logger.debug(f"Pattern details extracted: {len(details['pattern_matches'])} matches, "
                        f"{len(details['enhanced_patterns'])} pattern groups, "
                        f"{len(details['crisis_indicators'])} crisis indicators")
            
            return details
            
        except Exception as e:
            logger.error(f"Failed to extract pattern details: {e}")
            return {}

    def _extract_detailed_pattern_analysis(self, pattern_result: Dict) -> Dict[str, Any]:
        """
        ENHANCED: Extract detailed pattern analysis results with debug information
        """
        try:
            if not isinstance(pattern_result, dict):
                return self._get_empty_pattern_analysis()
            
            # Get the enhanced details from our improved extraction
            details = pattern_result.get('details', {})
            
            # Debug log the structure we're working with
            logger.debug(f"ðŸ” _extract_detailed_pattern_analysis - details keys: {list(details.keys()) if details else 'empty details'}")
            
            # Extract pattern details - using the corrected structure
            enhanced_patterns = details.get('enhanced_patterns', {})
            community_patterns = details.get('community_patterns', [])
            matches = details.get('pattern_matches', [])
            crisis_indicators = details.get('crisis_indicators', [])
            safety_flags = details.get('safety_flags', {})
            
            # If details is empty, try extracting directly from raw_result
            if not details:
                raw_result = pattern_result.get('raw_result', {})
                if raw_result:
                    logger.debug(f"ðŸ” Trying extraction from raw_result with keys: {list(raw_result.keys())}")
                    # Try to extract from the raw result structure
                    patterns_found = raw_result.get('patterns_found', [])
                    if patterns_found:
                        matches = patterns_found
                        logger.debug(f"ðŸ” Extracted {len(matches)} patterns from raw_result.patterns_found")
                        
                        # Build enhanced patterns from patterns_found
                        for pattern in patterns_found:
                            if isinstance(pattern, dict):
                                group = pattern.get('pattern_group', '')
                                if group:
                                    if group not in enhanced_patterns:
                                        enhanced_patterns[group] = []
                                    enhanced_patterns[group].append({
                                        'text': pattern.get('pattern_name', ''),
                                        'level': pattern.get('crisis_level', 'unknown'),
                                        'confidence': pattern.get('weight', 0.0)
                                    })
                                    
                                    # Add high-level patterns to crisis indicators
                                    level = pattern.get('crisis_level', 'unknown')
                                    if level in ['high', 'critical'] and group not in crisis_indicators:
                                        crisis_indicators.append(group)
                    
                    # Extract safety flags from raw result
                    raw_safety_flags = raw_result.get('safety_flags', {})
                    if raw_safety_flags:
                        safety_flags = raw_safety_flags
                        critical_patterns = raw_safety_flags.get('critical_patterns_detected', [])
                        crisis_indicators.extend(critical_patterns)
            
            # Remove duplicates
            crisis_indicators = list(set(crisis_indicators))
            
            logger.debug(f"ðŸ” Final pattern analysis structure:")
            logger.debug(f"ðŸ”   - Enhanced patterns: {list(enhanced_patterns.keys())}")
            logger.debug(f"ðŸ”   - Pattern matches: {len(matches)}")
            logger.debug(f"ðŸ”   - Crisis indicators: {crisis_indicators}")
            
            return {
                'enhanced_patterns': enhanced_patterns,
                'community_patterns': community_patterns,
                'matches': matches,
                'confidence': pattern_result.get('confidence', pattern_result.get('score', 0.0)),
                'severity_level': self._determine_pattern_severity(matches),
                'critical_patterns': crisis_indicators,
                'temporal_analysis': details.get('temporal_patterns', {}),
                'linguistic_features': details.get('context_patterns', {}),
                'severity_indicators': crisis_indicators[:5],  # Top 5 indicators
                'safety_flags': safety_flags
            }
            
        except Exception as e:
            logger.error(f"Failed to extract detailed pattern analysis: {e}")
            logger.exception("Pattern analysis extraction error:")
            return self._get_empty_pattern_analysis()

    def _extract_detailed_model_results(self, ensemble_result: Dict) -> Dict[str, Any]:
        """
        Extract detailed individual AI model results from ensemble analysis
        
        This captures the individual model scores being logged:
        - Depression model: score=0.910, final crisis score: 0.955  
        - Sentiment model: score=0.995, final crisis score: 0.997
        - Emotional distress model: score=0.709, final crisis score: 0.842
        """
        try:
            # Extract individual model results from ensemble_result
            individual_results = ensemble_result.get('individual_results', {})
            
            # If not found, try to reconstruct from available data
            if not individual_results and hasattr(self.analyzer, 'model_coordination_manager'):
                try:
                    # Try to get recent model results from coordination manager
                    manager = self.analyzer.model_coordination_manager
                    if hasattr(manager, '_last_classification_results'):
                        individual_results = getattr(manager, '_last_classification_results', {})
                except Exception as e:
                    logger.debug(f"Could not access recent model results: {e}")
            
            return {
                'individual_results': individual_results,
                'ensemble_method': ensemble_result.get('method', 'performance_optimized'),
                'agreement_score': self._calculate_model_agreement(individual_results),
                'ensemble_confidence': ensemble_result.get('confidence', 0.0)
            }
            
        except Exception as e:
            logger.error(f"Failed to extract detailed model results: {e}")
            return {
                'individual_results': {},
                'ensemble_method': 'error_fallback',
                'agreement_score': 0.0,
                'ensemble_confidence': 0.0
            }

    def _extract_individual_scores(self, model_results: Dict) -> Dict[str, float]:
        """Extract individual model scores for easy access"""
        try:
            individual_results = model_results.get('individual_results', {})
            scores = {}
            
            for model_name, result in individual_results.items():
                if isinstance(result, dict) and 'score' in result:
                    scores[model_name] = result['score']
            
            return scores
        except Exception as e:
            logger.error(f"Failed to extract individual scores: {e}")
            return {}

    def _extract_critical_patterns(self, matches: List) -> List[str]:
        """Extract critical pattern names from matches"""
        try:
            critical_patterns = []
            for match in matches:
                if isinstance(match, dict):
                    level = match.get('level', 'low')
                    if level in ['critical', 'high']:
                        pattern_name = match.get('pattern_group', match.get('pattern_type', 'unknown'))
                        if pattern_name and pattern_name not in critical_patterns:
                            critical_patterns.append(pattern_name)
            
            return critical_patterns
            
        except Exception as e:
            logger.error(f"Failed to extract critical patterns: {e}")
            return []

    def _extract_severity_indicators(self, matches: List) -> List[str]:
        """Extract severity indicators from pattern matches"""
        try:
            indicators = []
            for match in matches:
                if isinstance(match, dict):
                    # Extract pattern group/type
                    pattern_group = match.get('pattern_group', '')
                    if pattern_group:
                        indicators.append(pattern_group)
                    
                    # Extract specific patterns
                    pattern_type = match.get('pattern_type', '')
                    if pattern_type and pattern_type not in indicators:
                        indicators.append(pattern_type)
            
            # Remove duplicates and return top 5
            return list(set(indicators))[:5]
            
        except Exception as e:
            logger.error(f"Failed to extract severity indicators: {e}")
            return []
    
    def _extract_categories(self, ensemble_result: Dict, pattern_result: Dict) -> List[str]:
        """Extract detected crisis categories quickly"""
        categories = ['automated_analysis']
        
        # Add ensemble-based categories
        if ensemble_result.get('score', 0.0) > 0.3:
            categories.append('ai_detected')
        
        # Add pattern-based categories  
        if pattern_result.get('score', 0.0) > 0.3:
            categories.append('pattern_detected')
        
        return categories

    def _extract_temporal_factors_for_scoring(self, pattern_result: Dict[str, Any], message: str) -> List[Dict[str, Any]]:
        """
        NEW METHOD: Extract temporal factors specifically for scoring application
        
        This method extracts temporal factors from pattern analysis results and
        ensures they're in the correct format for score adjustment.
        
        Args:
            pattern_result: Pattern analysis result from pattern detection
            message: Original message for direct extraction if needed
            
        Returns:
            List of temporal factors ready for score adjustment
        """
        try:
            temporal_factors = []
            
            # Try to get from existing extraction method
            logger.debug("Standard temporal factor attempt ...")

            try:
                extracted_factors = self._extract_temporal_factors({}, pattern_result, message)
                if extracted_factors:
                    temporal_factors.extend(extracted_factors)
            except Exception as e:
                logger.debug(f"Standard temporal factor extraction failed: {e}")
            
            # If no factors found, try direct pattern result examination
            if not temporal_factors:
                logger.debug("Direct pattern temporal factor attempt ...")

                try:
                    # Check pattern result details for temporal information
                    details = pattern_result.get('details', {})
                    temporal_analysis = details.get('temporal_analysis', {})
                    
                    if temporal_analysis and 'found_indicators' in temporal_analysis:
                        for indicator in temporal_analysis['found_indicators']:
                            temporal_factors.append({
                                'indicator_type': indicator.get('indicator_type', 'unknown'),
                                'crisis_boost': indicator.get('boost_factor', 0.0),
                                'urgency_score': indicator.get('urgency_score', 0.0),
                                'matched_phrase': indicator.get('matched_phrase', ''),
                                'temporal_category': indicator.get('temporal_category', 'general')
                            })
                            
                except Exception as e:
                    logger.debug(f"Direct temporal extraction from pattern result failed: {e}")
            
            # If still no factors, try direct pattern detection manager call
            if not temporal_factors and hasattr(self.analyzer, 'pattern_detection_manager'):
                logger.debug("Pattern detection manager attempt ...")

                try:
                    temporal_analysis = self.analyzer.pattern_detection_manager.analyze_temporal_indicators(message)
                    
                    if temporal_analysis and 'found_indicators' in temporal_analysis:
                        for indicator in temporal_analysis['found_indicators']:
                            temporal_factors.append({
                                'indicator_type': indicator.get('indicator_type', 'unknown'),
                                'crisis_boost': indicator.get('boost_factor', 0.0),
                                'urgency_score': indicator.get('urgency_score', 0.0),
                                'matched_phrase': indicator.get('matched_phrase', ''),
                                'temporal_category': indicator.get('temporal_category', 'general')
                            })
                            
                except Exception as e:
                    logger.debug(f"Direct pattern detection manager temporal analysis failed: {e}")
            
            logger.debug(f"ðŸ• Extracted {len(temporal_factors)} temporal factors for scoring")
            return temporal_factors
            
        except Exception as e:
            logger.error(f"Failed to extract temporal factors for scoring: {e}")
            return []
    # ========================================================================

    # ========================================================================
    # HELPERS
    # ========================================================================
    def _get_model_labels(self, model_type: str) -> List[str]:
        """Get labels for model type (optimized)"""
        label_sets = {
            'depression': [
                "person expressing suicidal thoughts or plans",
                "person showing severe depression", 
                "person feeling emotionally stable"
            ],
            'sentiment': [
                "extreme sadness or despair",
                "neutral emotions",
                "happiness or joy"
            ],
            'emotional_distress': [
                "person in acute psychological distress",
                "person showing moderate distress",
                "person demonstrating emotional resilience"
            ]
        }
        return label_sets.get(model_type, ["high crisis", "medium crisis", "low crisis"])

    def _get_feature_flags(self) -> Dict[str, bool]:
        """
        Get feature flags from feature_config_manager with resilient fallbacks
        Following Clean Architecture Charter Rule #5
        
        ADD THIS METHOD to PerformanceOptimizedMethods class
        """
        try:
            if self.analyzer.feature_config_manager:
                return {
                    'advanced_context': self.analyzer.feature_config_manager.is_advanced_context_enabled(),
                    'community_vocab': self.analyzer.feature_config_manager.is_community_vocab_enabled(),
                    'temporal_patterns': self.analyzer.feature_config_manager.is_temporal_patterns_enabled(),
                    'pattern_analysis': self.analyzer.feature_config_manager.is_pattern_analysis_enabled(),
                    'context_analysis': self.analyzer.feature_config_manager.is_context_analysis_enabled()
                }
            else:
                logger.warning("Feature config manager not available, using safe defaults")
                return self._get_fallback_feature_flags()
                
        except Exception as e:
            logger.error(f"Failed to get feature flags: {e}, using safe defaults")
            return self._get_fallback_feature_flags()

    def _build_conditional_pattern_analysis(self, detailed_patterns: Dict[str, Any], feature_flags: Dict[str, bool]) -> Dict[str, Any]:
        """
        NEW METHOD: Build pattern analysis section conditionally based on feature flags
        
        This is the core fix for the advanced features issue:
        - enhanced_patterns populated only if advanced_context is enabled  
        - community_patterns populated only if community_vocab is enabled
        - Other fields populated based on standard pattern analysis being enabled
        
        ADD THIS METHOD to PerformanceOptimizedMethods class
        """
        try:
            pattern_analysis = {}
            
            # Always include basic pattern info if pattern analysis is enabled
            if feature_flags.get('pattern_analysis', True):
                pattern_analysis.update({
                    'pattern_matches': detailed_patterns.get('matches', []),
                    'pattern_confidence': detailed_patterns.get('confidence', 0.0),
                    'pattern_severity': detailed_patterns.get('severity_level', 'none'),
                    'critical_patterns': detailed_patterns.get('critical_patterns', [])
                })
            
            # ADVANCED CONTEXT FEATURE: Enhanced patterns only if enabled
            if feature_flags.get('advanced_context', False):
                enhanced_patterns = detailed_patterns.get('enhanced_patterns', {})
                if enhanced_patterns:
                    pattern_analysis['enhanced_patterns'] = enhanced_patterns
                    logger.debug(f"Advanced context enabled: populated enhanced_patterns with {len(enhanced_patterns)} groups")
                else:
                    pattern_analysis['enhanced_patterns'] = {}
                    logger.debug("Advanced context enabled but no enhanced patterns found")
            else:
                pattern_analysis['enhanced_patterns'] = {}
                logger.debug("Advanced context disabled: enhanced_patterns set to empty")
            
            # COMMUNITY VOCAB FEATURE: Community patterns only if enabled  
            if feature_flags.get('community_vocab', False):
                community_patterns = detailed_patterns.get('community_patterns', [])
                if community_patterns:
                    pattern_analysis['community_patterns'] = community_patterns
                    logger.debug(f"Community vocab enabled: populated community_patterns with {len(community_patterns)} patterns")
                else:
                    pattern_analysis['community_patterns'] = []
                    logger.debug("Community vocab enabled but no community patterns found")
            else:
                pattern_analysis['community_patterns'] = []
                logger.debug("Community vocab disabled: community_patterns set to empty")
            
            return pattern_analysis
            
        except Exception as e:
            logger.error(f"Failed to build conditional pattern analysis: {e}")
            # Resilient fallback per Clean Architecture Charter Rule #5
            return {
                'enhanced_patterns': {},
                'community_patterns': [],
                'pattern_matches': [],
                'pattern_confidence': 0.0,
                'pattern_severity': 'none',
                'critical_patterns': []
            }

    def _build_conditional_context_analysis(self, detailed_patterns: Dict[str, Any], pattern_result: Dict[str, Any], feature_flags: Dict[str, bool], message: str) -> Dict[str, Any]:
        """
        NEW METHOD: Build context analysis section conditionally based on feature flags
        
        This is the core fix for temporal_factors and other context fields:
        - temporal_factors populated only if temporal_patterns is enabled
        - linguistic_indicators populated only if advanced_context is enabled  
        - severity_indicators populated based on pattern analysis
        
        ADD THIS METHOD to PerformanceOptimizedMethods class
        """
        try:
            context_analysis = {}
            
            # TEMPORAL PATTERNS FEATURE: Temporal factors only if enabled
            if feature_flags.get('temporal_patterns', False):
                temporal_factors = self._extract_temporal_factors(detailed_patterns, pattern_result, message)
                if temporal_factors:
                    context_analysis['temporal_factors'] = temporal_factors
                    logger.debug(f"Temporal patterns enabled: populated temporal_factors with {len(temporal_factors)} factors")
                else:
                    context_analysis['temporal_factors'] = []
                    logger.debug("Temporal patterns enabled but no temporal factors found")
            else:
                context_analysis['temporal_factors'] = []
                logger.debug("Temporal patterns disabled: temporal_factors set to empty")
            
            # ADVANCED CONTEXT FEATURE: Linguistic indicators only if enabled
            if feature_flags.get('advanced_context', False):
                linguistic_indicators = detailed_patterns.get('linguistic_features', [])
                if linguistic_indicators:
                    context_analysis['linguistic_indicators'] = linguistic_indicators
                    logger.debug(f"Advanced context enabled: populated linguistic_indicators with {len(linguistic_indicators)} indicators")
                else:
                    context_analysis['linguistic_indicators'] = []
                    logger.debug("Advanced context enabled but no linguistic indicators found")
            else:
                context_analysis['linguistic_indicators'] = []
                logger.debug("Advanced context disabled: linguistic_indicators set to empty")
            
            # PATTERN ANALYSIS FEATURE: Severity indicators if pattern analysis enabled
            if feature_flags.get('pattern_analysis', True):
                severity_indicators = detailed_patterns.get('severity_indicators', [])[:5]  # Top 5
                context_analysis['severity_indicators'] = severity_indicators
                if severity_indicators:
                    logger.debug(f"Pattern analysis enabled: populated severity_indicators with {len(severity_indicators)} indicators")
            else:
                context_analysis['severity_indicators'] = []
            
            return context_analysis
            
        except Exception as e:
            logger.error(f"Failed to build conditional context analysis: {e}")
            # Resilient fallback per Clean Architecture Charter Rule #5
            return {
                'temporal_factors': [],
                'linguistic_indicators': [],
                'severity_indicators': []
            }

    def _reconstruct_enhanced_patterns(self, patterns_found: List[Dict]) -> Dict[str, List[Dict]]:
        """
        FIXED: Reconstruct enhanced patterns from patterns_found array with nested structure support
        
        This fixes the issue where pattern_group is nested inside a 'details' object
        
        REPLACE THIS METHOD in PerformanceOptimizedMethods class
        """
        try:
            enhanced_patterns = {}
            
            for pattern in patterns_found:
                if not isinstance(pattern, dict):
                    continue
                    
                # Try multiple ways to extract pattern information
                pattern_group = None
                pattern_level = None
                pattern_text = None
                confidence = 0.0
                
                # Method 1: Check top level first
                pattern_group = pattern.get('pattern_group')
                pattern_level = pattern.get('level', pattern.get('crisis_level', pattern.get('urgency')))
                pattern_text = pattern.get('text', pattern.get('pattern_name', pattern.get('matched_text', pattern.get('matched_pattern'))))
                confidence = pattern.get('confidence', pattern.get('weight', 0.0))
                
                # Method 2: Check nested in 'details' object (THIS IS THE KEY FIX)
                if not pattern_group and 'details' in pattern:
                    details = pattern['details']
                    if isinstance(details, dict):
                        pattern_group = details.get('pattern_group')
                        pattern_level = pattern_level or details.get('level', details.get('crisis_level', details.get('urgency')))
                        pattern_text = pattern_text or details.get('text', details.get('pattern_name', details.get('matched_text', details.get('matched_pattern'))))
                        confidence = confidence or details.get('confidence', details.get('weight', 0.0))
                        
                        logger.debug(f"ðŸ”§ Extracted from nested details: group={pattern_group}, text={pattern_text}, level={pattern_level}")
                
                # Method 3: Fallback to pattern_type as group
                if not pattern_group:
                    pattern_group = pattern.get('pattern_type', pattern.get('category'))
                
                # Only proceed if we have at least a pattern group
                if pattern_group:
                    # Add to enhanced patterns by group
                    if pattern_group not in enhanced_patterns:
                        enhanced_patterns[pattern_group] = []
                        
                    enhanced_patterns[pattern_group].append({
                        'text': pattern_text or 'unknown',
                        'level': pattern_level or 'unknown',
                        'confidence': float(confidence) if confidence else 0.0
                    })
                    
                    logger.debug(f"ðŸ”§ Added to enhanced_patterns[{pattern_group}]: {pattern_text} (level: {pattern_level})")
            
            logger.debug(f"ðŸ”§ Reconstructed enhanced patterns: {list(enhanced_patterns.keys())} with {sum(len(v) for v in enhanced_patterns.values())} total patterns")
            return enhanced_patterns
            
        except Exception as e:
            logger.error(f"Failed to reconstruct enhanced patterns: {e}")
            logger.exception("Pattern reconstruction error:")
            return {}

    def _get_empty_pattern_analysis(self) -> Dict[str, Any]:
        """Return empty pattern analysis structure"""
        return {
            'enhanced_patterns': {},
            'community_patterns': [],
            'matches': [],
            'confidence': 0.0,
            'severity_level': 'none',
            'critical_patterns': [],
            'temporal_analysis': {},
            'linguistic_features': {},
            'severity_indicators': []
        }

    def _calculate_model_agreement(self, individual_results: Dict) -> float:
        """Calculate agreement score between AI models"""
        try:
            if not individual_results or len(individual_results) < 2:
                return 0.0
            
            scores = []
            for model_result in individual_results.values():
                if isinstance(model_result, dict) and 'score' in model_result:
                    scores.append(model_result['score'])
            
            if len(scores) < 2:
                return 0.0
            
            # Calculate variance - lower variance = higher agreement
            mean_score = sum(scores) / len(scores)
            variance = sum((score - mean_score) ** 2 for score in scores) / len(scores)
            
            # Convert variance to agreement score (0-1, where 1 = perfect agreement)
            agreement = max(0.0, 1.0 - (variance * 4))  # Scale factor of 4
            return min(1.0, agreement)
            
        except Exception as e:
            logger.error(f"Failed to calculate model agreement: {e}")
            return 0.0

    def _determine_pattern_severity(self, matches: List) -> str:
        """Determine overall pattern severity level"""
        try:
            if not matches:
                return 'none'
            
            severity_levels = []
            for match in matches:
                if isinstance(match, dict):
                    level = match.get('level', 'low')
                    severity_levels.append(level)
            
            # Return highest severity found
            if 'critical' in severity_levels:
                return 'critical'
            elif 'high' in severity_levels:
                return 'high'
            elif 'medium' in severity_levels:
                return 'medium'
            elif 'low' in severity_levels:
                return 'low'
            else:
                return 'none'
                
        except Exception as e:
            logger.error(f"Failed to determine pattern severity: {e}")
            return 'none'
    # ========================================================================

    # ========================================================================
    # DEBUG AND ERROR HANDLING
    # ========================================================================
    def _debug_pattern_flow(self, detailed_patterns: Dict[str, Any]) -> Dict[str, Any]:
        """Debug method to trace pattern flow and force reconstruction if needed"""
        try:
            logger.info(f"DEBUG: Enhanced patterns before enhancement: {detailed_patterns.get('enhanced_patterns', 'MISSING')}")
            logger.info(f"DEBUG: Pattern matches count: {len(detailed_patterns.get('pattern_matches', []))}")
            
            current_enhanced = detailed_patterns.get('enhanced_patterns', {})
            pattern_matches = detailed_patterns.get('pattern_matches', [])
            
            # Force reconstruction if enhanced_patterns is empty but we have pattern_matches
            if (not current_enhanced or len(current_enhanced) == 0) and pattern_matches:
                logger.info("DEBUG: Forcing pattern reconstruction due to empty enhanced_patterns")
                reconstructed = self._reconstruct_enhanced_patterns(pattern_matches)
                detailed_patterns['enhanced_patterns'] = reconstructed
                logger.info(f"DEBUG: Reconstructed enhanced patterns: {list(reconstructed.keys())}")
            
            return detailed_patterns
            
        except Exception as e:
            logger.error(f"Debug pattern flow failed: {e}")
            return detailed_patterns
    
    def _create_error_response(self, error: str, message: str, user_id: str, channel_id: str, start_time: float) -> Dict[str, Any]:
        """Create standardized error response"""
        return {
            'crisis_score': 0.5,  # Conservative default
            'crisis_level': 'medium',  # Safe default
            'confidence_score': 0.3,
            'message': message,
            'user_id': user_id,
            'channel_id': channel_id,
            'method': 'error_fallback',
            'error': error,
            'needs_response': True,  # Safe default
            'requires_staff_review': True,  # Safe default
            'detected_categories': ['error', 'fallback'],
            'processing_time': (time.time() - start_time) * 1000,
            'optimization_version': 'v3.1-3e-7-1'
        }
    # ========================================================================

# ========================================================================
# FACTORY FUNCTION
# ========================================================================
def integrate_performance_optimizations(crisis_analyzer) -> PerformanceOptimizedMethods:
    """
    Integration function to add performance optimizations to CrisisAnalyzer
    
    Args:
        crisis_analyzer: CrisisAnalyzer instance to enhance
        
    Returns:
        PerformanceOptimizedMethods instance
    """
    try:
        optimizer = PerformanceOptimizedMethods(crisis_analyzer)
        logger.info("ðŸš€ Performance optimizations integrated successfully")
        return optimizer
    except Exception as e:
        logger.error(f"âŒ Performance optimization integration failed: {e}")
        raise
# ========================================================================

# ========================================================================
# PUBLIC INTERFACE
# ========================================================================
__all__ = [
    'PerformanceOptimizedMethods',
    'integrate_performance_optimizations'
]

logger.info("âœ… Performance optimizations module loaded - targeting 300ms analysis time")