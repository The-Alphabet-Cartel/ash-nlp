# ash-nlp/managers/model_ensemble_manager.py
"""
Model Ensemble Manager for Ash NLP Service v3.1
Clean v3.1 Architecture
Repository: https://github.com/the-alphabet-cartel/ash-nlp
Community: The Alphabet Cartel - https://discord.gg/alphabetcartel | https://alphabetcartel.org
"""

import os
import json
import logging
import re
import time
from typing import Dict, Any, List, Union, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

class ModelEnsembleManager:
    """
    Model Ensemble Manager with Phase 3d standardized variable support
    Updated to use enhanced UnifiedConfigManager with unified configuration approach
    """
    
    def __init__(self, config_manager):
        """
        Initialize Model Ensemble Manager
        
        Args:
            config_manager: Enhanced UnifiedConfigManager instance (Phase 3d)
        """
        if config_manager is None:
            raise ValueError("UnifiedConfigManager is required for ModelEnsembleManager")
        
        self.config_manager = config_manager
        self.config = None
        
        logger.info("‚úÖ ModelEnsembleManager v3.1d initialized with UnifiedConfigManager")
        
        # Load configuration using UnifiedConfigManager
        self._load_configuration()
        
        # Validate configuration
        self._validate_configuration()
    
    def _load_configuration(self):
        """Load model ensemble configuration using UnifiedConfigManager"""
        try:
            # Use UnifiedConfigManager's get_model_configuration method
            self.config = self.config_manager.get_model_configuration()
            
            if not self.config:
                raise ValueError("Model configuration could not be loaded")
            
            logger.info(f"‚úÖ Model ensemble configuration loaded via UnifiedConfigManager")
            logger.debug(f"üîç Loaded {len(self.config.get('models', {}))} model definitions")
            
            # Log standardized variables being used
            models = self.config.get('models', {})
            for model_type, model_config in models.items():
                logger.debug(f"   {model_type}: {model_config.get('name')} (weight: {model_config.get('weight')})")
        
        except Exception as e:
            logger.error(f"‚ùå Failed to load model configuration: {e}")
            raise
    
    def _validate_configuration(self):
        """
        Validate model ensemble configuration - FIXED WEIGHT VALIDATION
        
        This method validates the loaded configuration and logs any issues.
        Phase 3d: Enhanced validation with proper error handling
        """
        try:
            if not self.config:
                logger.error("‚ùå No configuration loaded")
                return False
            
            models = self.config.get('models', {})
            if not models:
                logger.warning("‚ö†Ô∏è No models configured")
                return False
            
            logger.debug(f"üîç Validating {len(models)} model definitions...")
            
            # Validate individual models
            valid_models = 0
            total_weight = 0.0
            
            for model_type, model_config in models.items():
                try:
                    # Check model name
                    model_name = model_config.get('name', '')
                    if not model_name or not model_name.strip():
                        logger.warning(f"   ‚ö†Ô∏è {model_type}: No model name configured")
                        continue
                    
                    # Check weight - FIXED: Proper type handling
                    weight = model_config.get('weight')
                    if weight is not None:
                        try:
                            # Convert to float if it's a string number
                            if isinstance(weight, str):
                                weight = float(weight)
                            elif isinstance(weight, (int, float)):
                                weight = float(weight)
                            else:
                                logger.warning(f"   ‚ö†Ô∏è {model_type}: Invalid weight type {type(weight)}: {weight}")
                                continue
                            
                            # Update the config with the converted weight
                            model_config['weight'] = weight
                            total_weight += weight
                            
                        except (ValueError, TypeError) as e:
                            logger.warning(f"   ‚ö†Ô∏è {model_type}: Could not convert weight '{weight}' to float: {e}")
                            continue
                    else:
                        logger.warning(f"   ‚ö†Ô∏è {model_type}: No weight configured")
                        continue
                    
                    logger.debug(f"   ‚úÖ {model_type}: {model_name} (weight: {weight})")
                    valid_models += 1
                    
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è {model_type}: Validation error: {e}")
                    continue
            
            # Overall validation results
            if valid_models == 0:
                logger.error("‚ùå No valid models found")
                return False
            
            # Weight validation - LENIENT
            if total_weight <= 0:
                logger.warning(f"‚ùå Invalid total weight: {total_weight}")
                return False
            
            # Check if weights are reasonable (lenient check)
            weight_deviation = abs(total_weight - 1.0)
            if weight_deviation > 0.5:  # Very lenient - allow up to 50% deviation
                logger.warning(f"‚ö†Ô∏è Weights sum to {total_weight:.3f}, ideally should be ~1.0, but continuing...")
            elif weight_deviation > 0.1:  # Moderate deviation
                logger.info(f"‚ÑπÔ∏è Weights sum to {total_weight:.3f}, close to ideal 1.0")
            else:
                logger.debug(f"‚úÖ Weights sum to {total_weight:.3f} - excellent")
            
            logger.info(f"‚úÖ Configuration validation passed: {valid_models}/{len(models)} models valid")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Configuration validation failed: {e}")
            logger.exception("Full validation error details:")
            return False
    
    def analyze_with_ensemble(self, message: str) -> Dict[str, Any]:
        """
        Three Zero-Shot Model Ensemble Analysis - REQUIRED BY CRISIS ANALYZER
        
        This method is specifically expected by CrisisAnalyzer and provides
        the three-model ensemble analysis that the system is designed to use.
        
        Args:
            message: Text message to analyze
            
        Returns:
            Dictionary containing ensemble analysis results with consensus data
        """
        try:
            logger.debug(f"üîç Three Zero-Shot Model Ensemble Analysis starting...")
            start_time = time.time()
            
            # Check if models are properly loaded
            if not self.models_loaded():
                logger.warning("‚ö†Ô∏è Models not loaded, cannot perform ensemble analysis")
                return self._create_ensemble_fallback_result(message, "Models not loaded")
            
            # Get model definitions
            models = self.get_model_definitions()
            if len(models) < 3:
                logger.warning(f"‚ö†Ô∏è Expected 3 models for ensemble, found {len(models)}")
                return self._create_ensemble_fallback_result(message, f"Insufficient models: {len(models)}")
            
            # Perform analysis with each model (simulated for now - Phase 3d placeholder)
            model_results = {}
            total_confidence = 0.0
            
            for model_type, model_config in models.items():
                try:
                    # Use the existing zero-shot classification method
                    model_name = model_config.get('name', '')
                    if model_name:
                        # Simulate analysis for crisis detection
                        confidence = self._analyze_text_with_model(message, model_type)
                        crisis_prediction = self._determine_crisis_from_confidence(confidence)
                        
                        model_results[model_type] = {
                            'model_name': model_name,
                            'prediction': crisis_prediction,
                            'confidence': confidence,
                            'weight': model_config.get('weight', 0.33)
                        }
                        total_confidence += confidence
                        
                        logger.debug(f"   üìä {model_type}: {crisis_prediction} (conf: {confidence:.3f})")
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error analyzing with {model_type}: {e}")
                    model_results[model_type] = {
                        'prediction': 'unknown',
                        'confidence': 0.0,
                        'error': str(e)
                    }
            
            # Calculate consensus
            consensus = self._calculate_ensemble_consensus(model_results)
            
            # Build ensemble result
            processing_time = (time.time() - start_time) * 1000
            
            ensemble_result = {
                'consensus': consensus,
                'individual_models': model_results,
                'ensemble_info': {
                    'total_models': len(models),
                    'successful_models': len([r for r in model_results.values() if 'error' not in r]),
                    'average_confidence': total_confidence / len(models) if models else 0.0
                },
                'detected_categories': self._extract_detected_categories(model_results),
                'model_info': f"Three Zero-Shot Model Ensemble ({len(models)} models)",
                'processing_time_ms': processing_time,
                'method': 'three_model_ensemble_v3d',
                'architecture': 'clean_v3_1'
            }
            
            logger.debug(f"‚úÖ Ensemble analysis complete: {consensus.get('prediction')} (conf: {consensus.get('confidence', 0):.3f})")
            return ensemble_result
            
        except Exception as e:
            logger.error(f"‚ùå Error in ensemble analysis: {e}")
            logger.exception("Full ensemble error details:")
            return self._create_ensemble_fallback_result(message, str(e))

    def _analyze_text_with_model(self, text: str, model_type: str) -> float:
        """
        Analyze text with specific model type (Phase 3d placeholder implementation)
        
        Args:
            text: Text to analyze
            model_type: Type of model ('depression', 'sentiment', 'emotional_distress')
            
        Returns:
            Confidence score (0.0 to 1.0)
        """
        try:
            # Phase 3d: Enhanced placeholder analysis with better logic
            text_lower = text.lower()
            
            if model_type == 'depression':
                # Depression-specific keywords and patterns
                depression_indicators = [
                    'hopeless', 'worthless', 'depressed', 'sad', 'empty', 'numb',
                    'dont want to live', 'dont want to continue', 'give up', 'end it',
                    'no point', 'no reason', 'hate myself', 'burden'
                ]
                matches = sum(1 for indicator in depression_indicators if indicator in text_lower)
                base_score = min(matches * 0.2, 0.8)  # Cap at 0.8
                
                # Boost for critical phrases
                if any(critical in text_lower for critical in ['dont want to continue living', 'want to die', 'kill myself']):
                    base_score = max(base_score, 0.75)
                    
                return min(base_score + 0.1, 1.0)  # Small boost, cap at 1.0
                
            elif model_type == 'sentiment':
                # Sentiment analysis focusing on negative emotions
                negative_sentiment = [
                    'terrible', 'awful', 'horrible', 'miserable', 'devastated',
                    'destroyed', 'broken', 'shattered', 'lost', 'alone'
                ]
                matches = sum(1 for word in negative_sentiment if word in text_lower)
                return min(matches * 0.15 + 0.1, 0.8)  # More conservative
                
            elif model_type == 'emotional_distress':
                # Emotional distress indicators
                distress_indicators = [
                    'overwhelmed', 'cant cope', 'too much', 'breaking down',
                    'falling apart', 'suffocating', 'drowning', 'crushing'
                ]
                matches = sum(1 for indicator in distress_indicators if indicator in text_lower)
                return min(matches * 0.25 + 0.05, 0.85)
            
            # Fallback for unknown model types
            return 0.1
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error in placeholder analysis for {model_type}: {e}")
            return 0.0

    def _determine_crisis_from_confidence(self, confidence: float) -> str:
        """
        Determine crisis level from confidence score
        
        Args:
            confidence: Confidence score (0.0 to 1.0)
            
        Returns:
            Crisis level string
        """
        if confidence >= 0.7:
            return 'crisis'
        elif confidence >= 0.5:
            return 'mild_crisis'
        elif confidence >= 0.3:
            return 'negative'
        elif confidence >= 0.1:
            return 'concerning'
        else:
            return 'unknown'

    def _calculate_ensemble_consensus(self, model_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Calculate ensemble consensus from individual model results
        
        Args:
            model_results: Dictionary of results from each model
            
        Returns:
            Consensus prediction and confidence
        """
        try:
            # Get all predictions and confidences
            predictions = []
            confidences = []
            weights = []
            
            for model_type, result in model_results.items():
                if 'error' not in result:
                    predictions.append(result.get('prediction', 'unknown'))
                    confidences.append(result.get('confidence', 0.0))
                    weights.append(result.get('weight', 0.33))
            
            if not predictions:
                return {'prediction': 'unknown', 'confidence': 0.0, 'method': 'no_valid_results'}
            
            # Use majority voting with confidence weighting
            prediction_scores = {}
            for pred, conf, weight in zip(predictions, confidences, weights):
                weighted_score = conf * weight
                prediction_scores[pred] = prediction_scores.get(pred, 0) + weighted_score
            
            # Get consensus prediction
            consensus_prediction = max(prediction_scores.keys(), key=lambda k: prediction_scores[k])
            
            # Calculate consensus confidence
            total_weight = sum(weights)
            consensus_confidence = prediction_scores[consensus_prediction] / total_weight if total_weight > 0 else 0.0
            
            return {
                'prediction': consensus_prediction,
                'confidence': min(consensus_confidence, 1.0),  # Cap at 1.0
                'method': 'weighted_majority',
                'prediction_scores': prediction_scores,
                'total_models': len(predictions)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error calculating consensus: {e}")
            return {'prediction': 'unknown', 'confidence': 0.0, 'method': 'error', 'error': str(e)}

    def _extract_detected_categories(self, model_results: Dict[str, Any]) -> List[str]:
        """
        Extract detected categories from model results
        
        Args:
            model_results: Dictionary of results from each model
            
        Returns:
            List of detected category strings
        """
        categories = set()
        
        for model_type, result in model_results.items():
            prediction = result.get('prediction', 'unknown')
            confidence = result.get('confidence', 0.0)
            
            # Add categories based on predictions and confidence
            if prediction in ['crisis', 'mild_crisis'] and confidence >= 0.5:
                categories.add('mental_health_crisis')
            if prediction == 'crisis' and confidence >= 0.7:
                categories.add('high_risk')
            if model_type == 'depression' and confidence >= 0.6:
                categories.add('depression_indicators')
            if prediction == 'negative' and confidence >= 0.5:
                categories.add('negative_sentiment')
        
        return list(categories)

    def _create_ensemble_fallback_result(self, message: str, error_message: str) -> Dict[str, Any]:
        """
        Create fallback result when ensemble analysis fails
        
        Args:
            message: Original message
            error_message: Error description
            
        Returns:
            Fallback ensemble result
        """
        return {
            'consensus': {
                'prediction': 'unknown',
                'confidence': 0.0,
                'method': 'fallback'
            },
            'individual_models': {},
            'ensemble_info': {
                'total_models': 0,
                'successful_models': 0,
                'average_confidence': 0.0
            },
            'detected_categories': [],
            'model_info': 'Ensemble analysis failed',
            'processing_time_ms': 0.0,
            'method': 'ensemble_fallback',
            'error': error_message,
            'architecture': 'clean_v3_1'
        }

    async def analyze_message_ensemble(self, message: str, user_id: str = "unknown", channel_id: str = "unknown") -> Dict[str, Any]:
        """
        Analyze message using ensemble models - CORRECTED VERSION
        
        This method is required by the API endpoints but delegates to CrisisAnalyzer
        following Clean v3.1 Architecture principles
        
        Args:
            message: Message text to analyze
            user_id: User ID for context
            channel_id: Channel ID for context
            
        Returns:
            Dictionary containing ensemble analysis results
        """
        try:
            logger.debug(f"üîç ModelEnsembleManager delegating analysis to CrisisAnalyzer")
            
            # Import here to avoid circular imports
            from analysis.crisis_analyzer import CrisisAnalyzer
            
            try:
                # Get other required managers for CrisisAnalyzer using factory functions
                from managers.crisis_pattern_manager import create_crisis_pattern_manager
                from managers.analysis_parameters_manager import create_analysis_parameters_manager
                from managers.threshold_mapping_manager import create_threshold_mapping_manager
                from managers.feature_config_manager import create_feature_config_manager
                from managers.performance_config_manager import create_performance_config_manager
                
                # Create managers using factory functions (Clean v3.1 compliance)
                crisis_pattern_manager = create_crisis_pattern_manager(self.config_manager)
                analysis_parameters_manager = create_analysis_parameters_manager(self.config_manager)
                threshold_mapping_manager = create_threshold_mapping_manager(self.config_manager, self)
                feature_config_manager = create_feature_config_manager(self.config_manager)
                performance_config_manager = create_performance_config_manager(self.config_manager)
                
                # CORRECTED: Create CrisisAnalyzer with the correct parameters (no config_manager)
                # Based on analysis/__init__.py, the correct parameters are:
                crisis_analyzer = CrisisAnalyzer(
                    models_manager=self,  # ModelEnsembleManager acts as models_manager
                    crisis_pattern_manager=crisis_pattern_manager,
                    learning_manager=None,  # Optional
                    analysis_parameters_manager=analysis_parameters_manager,
                    threshold_mapping_manager=threshold_mapping_manager,
                    feature_config_manager=feature_config_manager,
                    performance_config_manager=performance_config_manager
                )
                
                # Delegate to CrisisAnalyzer's analyze_message method
                logger.debug(f"‚úÖ CrisisAnalyzer created with correct parameters, performing analysis...")
                result = await crisis_analyzer.analyze_message(message, user_id, channel_id)
                
                logger.debug(f"‚úÖ Ensemble analysis complete via CrisisAnalyzer delegation")
                return result
                
            except Exception as e:
                logger.error(f"‚ùå Failed to create CrisisAnalyzer or dependencies: {e}")
                logger.exception("Full error details:")
                # Fallback to basic response structure
                return self._create_fallback_analysis_result(message, str(e))
                
        except Exception as e:
            logger.error(f"‚ùå Error in analyze_message_ensemble: {e}")
            logger.exception("Full error details:")
            return self._create_fallback_analysis_result(message, str(e))

    def _create_fallback_analysis_result(self, message: str, error_message: str) -> Dict[str, Any]:
        """
        Create fallback analysis result when ensemble analysis fails
        
        Args:
            message: Original message
            error_message: Error description
            
        Returns:
            Dictionary with fallback analysis result
        """
        return {
            'needs_response': False,
            'crisis_level': 'none',
            'confidence_score': 0.0,
            'detected_categories': [],
            'method': 'ensemble_fallback_error',
            'processing_time_ms': 0.0,
            'model_info': 'ModelEnsembleManager fallback - CrisisAnalyzer unavailable',
            'reasoning': f"Ensemble analysis failed: {error_message}",
            'analysis': {
                'error': error_message,
                'fallback_used': True,
                'ensemble_available': False
            },
            'staff_review_required': True,  # Always require review on errors
            'ensemble_status': {
                'models_configured': len(self.get_model_definitions()),
                'error': error_message,
                'fallback_reason': 'crisis_analyzer_creation_failed'
            }
        }

    # ===============================================================================
    # Semantic Pattern Classification using zero Shot Models already loaded.
    # ===============================================================================

    def classify_zero_shot(self, text: str, hypothesis: str, model_type: str = None) -> float:
        """
        Perform zero-shot classification using natural language inference
        
        This method uses the loaded zero-shot models to determine if a text
        semantically matches a given hypothesis (pattern category).
        
        Args:
            text: Text to classify
            hypothesis: Hypothesis to test (e.g., "This expresses suicidal thoughts")
            model_type: Specific model to use (optional, will auto-select if None)
            
        Returns:
            Confidence score (0.0 to 1.0) that the text matches the hypothesis
        """
        try:
            # Find appropriate zero-shot model
            if not model_type:
                model_type = self._get_best_zero_shot_model()
            
            if not model_type:
                logger.warning("‚ö†Ô∏è No zero-shot classification models available")
                return 0.0
            
            model_config = self.get_model_config(model_type)
            model_name = model_config.get('name', '')
            
            if not model_name:
                logger.warning(f"‚ö†Ô∏è No model name configured for {model_type}")
                return 0.0
            
            logger.debug(f"üß† Zero-shot classification: '{text[:30]}...' vs '{hypothesis[:50]}...'")
            
            # This is where you'd integrate with transformers pipeline
            # For now, using a demo implementation
            score = self._demo_zero_shot_classification(text, hypothesis, model_name)
            
            logger.debug(f"üìä Classification score: {score:.3f}")
            return score
            
        except Exception as e:
            logger.error(f"‚ùå Error in zero-shot classification: {e}")
            return 0.0

    def _get_best_zero_shot_model(self) -> str:
        """
        Find the best available zero-shot classification model
        
        Returns:
            Model type name that supports zero-shot classification, or None
        """
        try:
            models = self.get_model_definitions()
            
            # Look for models configured for zero-shot classification
            for model_type, model_config in models.items():
                pipeline_task = model_config.get('pipeline_task', '')
                if pipeline_task == 'zero-shot-classification':
                    logger.debug(f"‚úÖ Found zero-shot model: {model_type}")
                    return model_type
            
            # Fallback: Look for NLI models (can be used for zero-shot)
            for model_type, model_config in models.items():
                model_name = model_config.get('name', '').lower()
                if 'nli' in model_name or 'mnli' in model_name:
                    logger.debug(f"‚úÖ Found NLI model for zero-shot: {model_type}")
                    return model_type
            
            logger.warning("‚ö†Ô∏è No suitable zero-shot classification models found")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Error finding zero-shot model: {e}")
            return None

    def _demo_zero_shot_classification(self, text: str, hypothesis: str, model_name: str) -> float:
        """
        Demo zero-shot classification - REPLACE with actual model integration
        
        This demonstrates the concept. In production, this would use:
        
        from transformers import pipeline
        classifier = pipeline('zero-shot-classification', model=model_name)
        result = classifier(text, [hypothesis])
        return result['scores'][0]
        
        Args:
            text: Text to classify
            hypothesis: Hypothesis to test
            model_name: Model name for classification
            
        Returns:
            Classification confidence score
        """
        try:
            text_lower = text.lower()
            hypothesis_lower = hypothesis.lower()
            
            logger.debug(f"üî¨ Demo classification with {model_name}")
            
            # Enhanced demo logic that provides realistic results
            if "suicide" in hypothesis_lower or "not wanting to live" in hypothesis_lower:
                # Suicidal ideation classification
                suicide_keywords = [
                    "don't want to live", "do not want to live", "dont want to live",
                    "want to die", "kill myself", "end my life", "suicide",
                    "not worth living", "better off dead", "ready to die",
                    "continue living", "keep going", "stay alive"
                ]
                
                # Check for strong indicators
                strong_matches = 0
                for keyword in suicide_keywords:
                    if keyword in text_lower:
                        if "don't" in keyword or "do not" in keyword or "dont" in keyword:
                            strong_matches += 2  # Negation patterns are stronger indicators
                        else:
                            strong_matches += 1
                
                if strong_matches >= 2:
                    return min(0.85 + (strong_matches * 0.05), 0.95)
                elif strong_matches >= 1:
                    return 0.75
            
            elif "hopeless" in hypothesis_lower:
                # Hopelessness classification
                hopeless_keywords = [
                    "hopeless", "no hope", "despair", "desperate",
                    "pointless", "meaningless", "give up", "nothing matters"
                ]
                
                matches = sum(1 for keyword in hopeless_keywords if keyword in text_lower)
                if matches >= 1:
                    return min(0.70 + (matches * 0.1), 0.90)
            
            elif "distress" in hypothesis_lower:
                # Emotional distress classification
                distress_keywords = [
                    "overwhelming", "can't cope", "breaking down", "falling apart",
                    "too much", "suffocating", "drowning", "crushing"
                ]
                
                matches = sum(1 for keyword in distress_keywords if keyword in text_lower)
                if matches >= 1:
                    return min(0.65 + (matches * 0.1), 0.85)
            
            # Default: no strong semantic match
            return 0.0
            
        except Exception as e:
            logger.error(f"‚ùå Error in demo classification: {e}")
            return 0.0

    def get_zero_shot_capabilities(self) -> Dict[str, Any]:
        """
        Get information about zero-shot classification capabilities
        
        Returns:
            Dictionary with zero-shot classification status and available models
        """
        try:
            zero_shot_model = self._get_best_zero_shot_model()
            
            capabilities = {
                'zero_shot_available': zero_shot_model is not None,
                'zero_shot_model': zero_shot_model,
                'semantic_pattern_matching': zero_shot_model is not None,
                'classification_method': 'transformers_pipeline' if zero_shot_model else 'keyword_fallback'
            }
            
            if zero_shot_model:
                model_config = self.get_model_config(zero_shot_model)
                capabilities['model_details'] = {
                    'name': model_config.get('name', ''),
                    'type': model_config.get('type', ''),
                    'pipeline_task': model_config.get('pipeline_task', '')
                }
            
            return capabilities
            
        except Exception as e:
            logger.error(f"‚ùå Error getting zero-shot capabilities: {e}")
            return {'zero_shot_available': False, 'error': str(e)}

    # ========================================================================
    # Model Configuration Access - Phase 3d Enhanced
    # ========================================================================
    
    def get_model_definitions(self) -> Dict[str, Any]:
        """Get all model definitions with standardized variable support"""
        return self.config.get('models', {})
    
    def get_model_config(self, model_type: str) -> Dict[str, Any]:
        """Get configuration for specific model type"""
        models = self.get_model_definitions()
        return models.get(model_type, {})
    
    def get_model_name(self, model_type: str) -> str:
        """Get model name for specific model type"""
        model_config = self.get_model_config(model_type)
        return model_config.get('name', '')
    
    def get_model_weight(self, model_type: str) -> float:
        """Get model weight for specific model type"""
        model_config = self.get_model_config(model_type)
        return model_config.get('weight', 0.0)
    
    def get_model_cache_dir(self, model_type: str) -> str:
        """Get cache directory for specific model type"""
        model_config = self.get_model_config(model_type)
        return model_config.get('cache_dir', './models/cache')
    
    def get_model_pipeline_task(self, model_type: str) -> str:
        """Get pipeline task for specific model type"""
        model_config = self.get_model_config(model_type)
        return model_config.get('pipeline_task', 'text-classification')
    
    # ========================================================================
    # Ensemble Configuration - Phase 3d Enhanced  
    # ========================================================================
    
    def get_ensemble_mode(self) -> str:
        """Get current ensemble mode"""
        return self.config.get('ensemble_mode', 'consensus')
    
    def get_ensemble_settings(self) -> Dict[str, Any]:
        """Get ensemble settings including validation and performance"""
        return {
            'mode': self.get_ensemble_mode(),
            'validation': self.config.get('validation', {}),
            'performance': self.config.get('performance', {})
        }
    
    def get_current_ensemble_mode(self) -> str:
        """Get the currently configured ensemble mode - alias for compatibility"""
        return self.get_ensemble_mode()
    
    # ========================================================================
    # Hardware Configuration - Phase 3d Enhanced
    # ========================================================================
    
    def get_hardware_settings(self) -> Dict[str, Any]:
        """Get hardware configuration settings"""
        return self.config.get('hardware_settings', {})
    
    def get_device_setting(self) -> str:
        """Get device setting (auto/cpu/cuda)"""
        hardware = self.get_hardware_settings()
        return hardware.get('device', 'auto')
    
    def get_precision_setting(self) -> str:
        """Get precision setting (float16/float32)"""
        hardware = self.get_hardware_settings()
        return hardware.get('precision', 'float16')
    
    def get_max_batch_size(self) -> int:
        """Get maximum batch size"""
        hardware = self.get_hardware_settings()
        return hardware.get('max_batch_size', 32)
    
    def get_inference_threads(self) -> int:
        """Get inference thread count"""
        hardware = self.get_hardware_settings()
        return hardware.get('inference_threads', 16)
    
    # ========================================================================
    # Model Weight Management - Phase 3d Enhanced
    # ========================================================================
    
    def get_model_weights(self) -> Dict[str, float]:
        """Get all model weights as dictionary"""
        models = self.get_model_definitions()
        return {model_type: model.get('weight', 0.0) for model_type, model in models.items()}
    
    def get_normalized_weights(self) -> Dict[str, float]:
        """Get normalized model weights (sum to 1.0)"""
        weights = self.get_model_weights()
        total_weight = sum(weights.values())
        
        if total_weight <= 0:
            # Equal weights if all are zero
            equal_weight = 1.0 / len(weights) if weights else 0.0
            return {model_type: equal_weight for model_type in weights.keys()}
        
        # Normalize to sum to 1.0
        return {model_type: weight / total_weight for model_type, weight in weights.items()}
    
    def models_loaded(self) -> bool:
        """
        Check if models are loaded and ready for analysis - IMPROVED VERSION
        This method is required for API compatibility with ModelsManager interface
        
        More lenient validation that focuses on essential requirements
        
        Returns:
            bool: True if models are configured and ready, False otherwise
        """
        try:
            # Check if we have model definitions
            models = self.get_model_definitions()
            if not models:
                logger.warning("‚ùå No models configured in model definitions")
                return False
            
            logger.debug(f"üîç Found {len(models)} model definitions: {list(models.keys())}")
            
            # Check if we have at least the core models (be more flexible about exact names)
            required_model_count = 2  # At least 2 models for basic functionality
            if len(models) < required_model_count:
                logger.warning(f"‚ùå Only {len(models)} models configured, need at least {required_model_count}")
                return False
            
            # Validate that models have names (essential requirement)
            models_with_names = 0
            for model_type, model_config in models.items():
                model_name = model_config.get('name', '')
                if model_name and model_name.strip():
                    models_with_names += 1
                    logger.debug(f"   ‚úÖ {model_type}: {model_name}")
                else:
                    logger.warning(f"   ‚ö†Ô∏è {model_type}: missing or empty name")
            
            if models_with_names == 0:
                logger.warning("‚ùå No models have valid names configured")
                return False
            
            # Check weights (be lenient - just ensure they exist and are reasonable)
            try:
                weights = self.get_model_weights()
                total_weight = sum(weights.values())
                
                logger.debug(f"üîç Model weights: {weights}")
                logger.debug(f"üîç Total weight: {total_weight}")
                
                # Be very lenient with weights - just check they're not zero
                if total_weight <= 0:
                    logger.warning(f"‚ùå Invalid total weight: {total_weight}")
                    return False
                
                # Allow weight tolerance up to 50% deviation (very lenient)
                weight_tolerance = 0.5
                if abs(total_weight - 1.0) > weight_tolerance:
                    logger.info(f"‚ö†Ô∏è Model weights sum to {total_weight}, ideally should be ~1.0, but continuing...")
                    # Don't fail - just log warning
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Could not validate weights: {e}, but continuing...")
                # Don't fail on weight validation errors
            
            logger.info(f"‚úÖ Models validation passed: {models_with_names}/{len(models)} models with valid names")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error checking models_loaded status: {e}")
            logger.exception("Full error details:")
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """
        Get comprehensive model information for API responses - IMPROVED VERSION
        This method is required for API compatibility with ModelsManager interface
        
        More robust error handling and comprehensive information
        
        Returns:
            Dict containing model configuration and status information
        """
        try:
            models = self.get_model_definitions()
            
            # Build model info response with error handling
            model_info = {
                'total_models': len(models),
                'models_configured': len(models) > 0,
                'architecture_version': '3.1d',
                'configuration_source': 'unified_config_manager',
                'model_details': {}
            }
            
            # Add ensemble mode safely
            try:
                model_info['ensemble_mode'] = self.get_ensemble_mode()
            except Exception as e:
                logger.warning(f"Could not get ensemble mode: {e}")
                model_info['ensemble_mode'] = 'unknown'
            
            # Add weights safely
            try:
                weights = self.get_model_weights()
                model_info['total_weight'] = sum(weights.values())
                model_info['weights_valid'] = abs(sum(weights.values()) - 1.0) < 0.5  # Lenient
            except Exception as e:
                logger.warning(f"Could not get model weights: {e}")
                model_info['total_weight'] = 0.0
                model_info['weights_valid'] = False
            
            # Add hardware settings safely
            try:
                model_info['device_setting'] = self.get_device_setting()
                model_info['precision_setting'] = self.get_precision_setting()
            except Exception as e:
                logger.warning(f"Could not get hardware settings: {e}")
                model_info['device_setting'] = 'unknown'
                model_info['precision_setting'] = 'unknown'
            
            # Add details for each model with comprehensive error handling
            for model_type, model_config in models.items():
                try:
                    model_info['model_details'][model_type] = {
                        'name': model_config.get('name', ''),
                        'weight': model_config.get('weight', 0.0),
                        'type': model_config.get('type', ''),
                        'pipeline_task': model_config.get('pipeline_task', 'text-classification'),
                        'cache_dir': model_config.get('cache_dir', './models/cache'),
                        'configured': bool(model_config.get('name', '').strip())
                    }
                except Exception as e:
                    logger.warning(f"Error processing model {model_type}: {e}")
                    model_info['model_details'][model_type] = {
                        'error': str(e),
                        'configured': False
                    }
            
            # Add final status
            model_info['status'] = {
                'models_loaded': self.models_loaded(),
                'ready_for_analysis': len(models) >= 2 and any(
                    details.get('configured', False) 
                    for details in model_info['model_details'].values()
                )
            }
            
            logger.debug(f"‚úÖ Model info generated successfully: {len(models)} models")
            return model_info
            
        except Exception as e:
            logger.error(f"‚ùå Error generating model info: {e}")
            logger.exception("Full error details:")
            return {
                'total_models': 0,
                'models_configured': False,
                'status': 'error',
                'error': str(e),
                'architecture_version': '3.1d',
                'ready_for_analysis': False
            }

    # ========================================================================
    # Validation and Utility Methods - Phase 3d Enhanced
    # ========================================================================
    
    def validate_ensemble_mode(self, mode: str) -> bool:
        """Validate if an ensemble mode is supported"""
        available_modes = ['consensus', 'majority', 'weighted']
        return mode in available_modes
    
    def get_model_names(self) -> List[str]:
        """Get list of configured model names"""
        return list(self.get_model_definitions().keys())
    
    def get_validation_settings(self) -> Dict[str, Any]:
        """Get validation settings"""
        return self.config.get('validation', {})
    
    def is_weights_validation_enabled(self) -> bool:
        """Check if weight validation is enabled"""
        validation = self.get_validation_settings()
        return validation.get('ensure_weights_sum_to_one', True)
    
    def should_fail_on_invalid_weights(self) -> bool:
        """Check if system should fail on invalid weights"""
        validation = self.get_validation_settings()
        return validation.get('fail_on_invalid_weights', True)
    
    # ========================================================================
    # Storage Configuration - Phase 3d Enhanced  
    # ========================================================================
    
    def get_storage_configuration(self) -> Dict[str, Any]:
        """Get storage configuration via UnifiedConfigManager"""
        try:
            return self.config_manager.get_storage_configuration()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not get storage configuration: {e}")
            return {
                'directories': {
                    'models_directory': './models/cache'
                }
            }
    
    def get_models_directory(self) -> str:
        """Get models directory from storage configuration"""
        storage_config = self.get_storage_configuration()
        directories = storage_config.get('directories', {})
        return directories.get('models_directory', './models/cache')
    
    # ========================================================================
    # Status and Information Methods - Phase 3d Enhanced
    # ========================================================================
    
    def get_manager_status(self) -> Dict[str, Any]:
        """Get comprehensive manager status"""
        try:
            models = self.get_model_definitions()
            weights = self.get_model_weights()
            
            return {
                'version': '3.1d-enhanced',
                'architecture': 'clean-v3.1-unified',
                'config_source': 'enhanced_config_manager',
                'ensemble_mode': self.get_ensemble_mode(),
                'models_configured': len(models),
                'model_types': list(models.keys()),
                'total_weight': sum(weights.values()),
                'weights_normalized': abs(sum(weights.values()) - 1.0) < 0.01,
                'hardware_device': self.get_device_setting(),
                'storage_directory': self.get_models_directory(),
                'validation_enabled': self.is_weights_validation_enabled()
            }
        except Exception as e:
            logger.error(f"‚ùå Error getting manager status: {e}")
            return {
                'version': '3.1d-enhanced',
                'status': 'error',
                'error': str(e)
            }
    
    def print_configuration_summary(self):
        """Print detailed configuration summary for debugging"""
        logger.info("=== ModelEnsembleManager v3.1d Configuration Summary ===")
        
        status = self.get_manager_status()
        for key, value in status.items():
            logger.info(f"{key}: {value}")
        
        logger.info("--- Model Details ---")
        models = self.get_model_definitions()
        for model_type, model_config in models.items():
            logger.info(f"{model_type}:")
            logger.info(f"  Name: {model_config.get('name')}")
            logger.info(f"  Weight: {model_config.get('weight')}")
            logger.info(f"  Type: {model_config.get('type')}")
            logger.info(f"  Pipeline Task: {model_config.get('pipeline_task')}")
        
        logger.info("=== End Configuration Summary ===")

# ============================================================================
# Global Instance Management - Singleton Pattern
# ============================================================================

_model_ensemble_manager = None

def get_model_ensemble_manager(config_manager=None) -> ModelEnsembleManager:
    """
    Get the global model ensemble manager instance - TRANSITION COMPATIBLE
    
    Args:
        config_manager: UnifiedConfigManager instance (optional for compatibility)
        
    Returns:
        ModelEnsembleManager instance
    """
    global _model_ensemble_manager
    
    if _model_ensemble_manager is None:
        # If no config_manager provided, create one (for backward compatibility)
        if config_manager is None:
            logger.info("üîÑ Creating UnifiedConfigManager for ModelEnsembleManager compatibility")
            from managers.unified_config_manager import UnifiedConfigManager
            config_manager = UnifiedConfigManager("/app/config")
        
        _model_ensemble_manager = ModelEnsembleManager(config_manager)
    
    return _model_ensemble_manager

def reset_model_ensemble_manager():
    """Reset the global manager instance - for testing"""
    global _model_ensemble_manager
    _model_ensemble_manager = None

# ============================================================================
# Factory Function - Clean v3.1 Architecture Compliance
# ============================================================================

def create_model_ensemble_manager(config_manager) -> ModelEnsembleManager:
    """
    Factory function to create ModelEnsembleManager instance
    
    Args:
        config_manager: UnifiedConfigManager instance
        
    Returns:
        ModelEnsembleManager instance
    """
    return ModelEnsembleManager(config_manager)

__all__ = [
    'ModelEnsembleManager', 
    'get_model_ensemble_manager', 
    'create_model_ensemble_manager',
    'reset_model_ensemble_manager'
]

logger.info("‚úÖ Enhanced ModelEnsembleManager v3.1d loaded - Phase 3d standardized variables supported")