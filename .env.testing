# Ash-NLP Testing Suite - Environment Variables
# FILE VERSION: v5.0
# LAST MODIFIED: 2025-12-30
# Repository: https://github.com/the-alphabet-cartel/ash-nlp
#
# INSTRUCTIONS:
# 1. Copy this file to .env in your project root
# 2. Adjust values as needed for your system
# 3. Uncomment variables you want to customize (most have good defaults)

# ==============================================================================
# TESTING EXECUTION CONFIGURATION
# ==============================================================================

# Device to use for testing (cuda = GPU, cpu = CPU)
# Default: cuda (will auto-fallback to cpu if GPU unavailable)
TEST_DEVICE=cuda

# Batch size for model inference
# Lower this if you get Out of Memory errors
# Default: 8
# Recommended for 12GB VRAM: 8
# Recommended for 8GB VRAM: 4
# Recommended for 4GB VRAM: 2
TEST_BATCH_SIZE=8

# Timeout for individual test cases (seconds)
# Default: 30
TEST_TIMEOUT=30

# Run tests in parallel (not yet implemented, reserved for future)
# Default: false
TEST_PARALLEL=false

# Verbose output during testing
# Default: true
TEST_VERBOSE=true

# ==============================================================================
# ACCURACY THRESHOLDS
# ==============================================================================
# These define the minimum acceptable performance metrics
# Tests will flag results below these thresholds

# Minimum overall accuracy (0.0 to 1.0)
# Default: 0.85 (85%)
TEST_ACCURACY_THRESHOLD=0.85

# Minimum precision (0.0 to 1.0)
# Default: 0.80 (80%)
TEST_PRECISION_THRESHOLD=0.80

# Minimum recall/sensitivity (0.0 to 1.0)
# Default: 0.85 (85%)
TEST_RECALL_THRESHOLD=0.85

# Minimum F1 score (0.0 to 1.0)
# Default: 0.82 (82%)
TEST_F1_THRESHOLD=0.82

# ==============================================================================
# PERFORMANCE THRESHOLDS
# ==============================================================================
# These define acceptable performance limits

# Maximum acceptable latency per inference (milliseconds)
# Default: 5000ms (5 seconds)
# Note: v5.0 ensemble may take up to 7-10 seconds for 4 models
TEST_LATENCY_THRESHOLD=5000

# Maximum acceptable VRAM usage (megabytes)
# Default: 2000MB (2GB)
# Note: v5.0 ensemble uses ~1.5GB total for all 4 models
TEST_VRAM_THRESHOLD=2000

# ==============================================================================
# TEST DATASET PATHS
# ==============================================================================
# Paths to test dataset files (relative to project root)
# Default paths assume standard testing/ directory structure

# Crisis examples dataset (55 cases: critical â†’ low severity)
TEST_DATASET_CRISIS=testing/test_datasets/crisis_examples.json

# Safe examples dataset (52 cases: metaphors, positive, neutral)
TEST_DATASET_SAFE=testing/test_datasets/safe_examples.json

# Edge cases dataset (50 cases: sarcasm, irony, ambiguous)
TEST_DATASET_EDGE=testing/test_datasets/edge_cases.json

# LGBTQIA+ specific dataset (50 cases: identity, dysphoria, community)
TEST_DATASET_LGBTQIA=testing/test_datasets/lgbtqia_specific.json

# Escalation patterns dataset (20 sequences: temporal patterns)
TEST_DATASET_ESCALATION=testing/test_datasets/escalation_patterns.json

# ==============================================================================
# MODEL CACHING
# ==============================================================================
# Where to cache downloaded models (speeds up subsequent runs)

# HuggingFace transformers cache directory
# Default: testing/cache/models (Docker: /app/testing/cache/models)
# Note: Models are ~400MB each, ensure sufficient disk space
TRANSFORMERS_CACHE=testing/cache/models

# HuggingFace home directory (stores tokens, configs)
# Default: testing/cache/huggingface (Docker: /app/testing/cache/huggingface)
HF_HOME=testing/cache/huggingface

# ==============================================================================
# REPORT OUTPUT CONFIGURATION
# ==============================================================================

# Directory for test report output
# Default: testing/reports/output
TEST_REPORT_OUTPUT_DIR=testing/reports/output

# Generate HTML reports (future feature)
# Default: true
TEST_GENERATE_HTML=true

# Generate JSON reports
# Default: true
TEST_GENERATE_JSON=true

# Save performance charts (future feature)
# Default: true
TEST_SAVE_CHARTS=true

# ==============================================================================
# BASELINE MODEL CONFIGURATION
# ==============================================================================
# Current v3.1 model for baseline comparison

# Baseline model identifier from HuggingFace
# Default: MoritzLaurer/deberta-v3-base-zeroshot-v2.0 (current v3.1 model)
TEST_BASELINE_MODEL=MoritzLaurer/deberta-v3-base-zeroshot-v2.0

# ==============================================================================
# LOGGING CONFIGURATION
# ==============================================================================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# Default: INFO
LOG_LEVEL=INFO

# Log file location
# Default: logs/testing.log
LOG_FILE=logs/testing.log

# ==============================================================================
# OPTIONAL: HUGGINGFACE AUTHENTICATION
# ==============================================================================
# Only needed if testing gated models (not required for v5.0)

# HuggingFace API token (for gated models)
# Get yours at: https://huggingface.co/settings/tokens
# HF_TOKEN=your_token_here

# ==============================================================================
# OPTIONAL: GPU-SPECIFIC SETTINGS
# ==============================================================================
# Advanced GPU configuration (usually not needed)

# Specific GPU device ID to use (0, 1, 2, etc.)
# Default: 0 (first GPU)
# CUDA_VISIBLE_DEVICES=0

# Enable TF32 for faster computation on Ampere+ GPUs
# Default: true (for RTX 3000+ series)
# CUDA_TF32=true

# ==============================================================================
# DOCKER-SPECIFIC VARIABLES
# ==============================================================================
# These are automatically set by docker-compose, but can be overridden

# Container shared memory size (for PyTorch DataLoader)
# Set in docker-compose.yml, but noted here for reference
# Default: 8gb (docker-compose sets this)

# ==============================================================================
# DEVELOPMENT/DEBUG SETTINGS
# ==============================================================================

# Enable debug mode (extra logging, no model caching)
# Default: false
# DEBUG_MODE=false

# Skip model downloads (use cached only - fails if not cached)
# Default: false
# OFFLINE_MODE=false

# Force CPU mode even if GPU available (for debugging)
# Default: false
# FORCE_CPU=false

# ==============================================================================
# NOTES
# ==============================================================================
#
# Memory Requirements:
# - Minimum: 8GB RAM + 4GB VRAM (CPU fallback available)
# - Recommended: 16GB RAM + 8GB VRAM
# - Optimal: 32GB+ RAM + 12GB VRAM (like Lofn server)
#
# Disk Space:
# - Models cache: ~2-3GB for all v5.0 models
# - Test reports: ~10-50MB per comprehensive test
# - Docker image: ~8GB
#
# Performance Expectations:
# - GPU (RTX 3060): ~3-7 seconds per test case
# - CPU (Ryzen 7 5800x): ~15-30 seconds per test case
#
# First Run:
# - Models will download automatically (~2-3GB total)
# - Subsequent runs use cached models (much faster)
#
# Troubleshooting:
# - Out of Memory: Reduce TEST_BATCH_SIZE to 4, 2, or 1
# - Slow Performance: Verify TEST_DEVICE=cuda and GPU is detected
# - Model Download Fails: Check internet connection or set OFFLINE_MODE=true
#
# Support:
# - Documentation: testing/README.md, DOCKER_README.md
# - Discord: https://discord.gg/alphabetcartel
# - Issues: https://github.com/the-alphabet-cartel/ash-nlp/issues
